Chaos, Solitons and Fractals 91 (2016) 549–553
Contents lists available at ScienceDirect
Chaos, Solitons and Fractals
Nonlinear Science, and Nonequilibrium and Complex Phenomena journal homepage: www.elsevier.com/locate/chaos

Deng entropy
Yong Deng
Institute of Integrated Automation, School of Electronic and Information Engineering, Xi’an Jiaotong University, Xian, Shaanxi, 710049, China

article info
Article history: Received 15 April 2016 Revised 4 June 2016 Accepted 28 July 2016
Keywords: Uncertainty measure Entropy Deng entropy Shannon entropy Dempster-Shafer evidence theory

a b s t r a c t
Dempster Shafer evidence theory has been widely used in many applications due to its advantages to handle uncertainty. However, how to measure uncertainty in evidence theory is still an open issue. The main contribution of this paper is that a new entropy, named as Deng entropy, is presented to measure the uncertainty of a basic probability assignment (BPA). Deng entropy is the generalization of Shannon entropy since the value of Deng entropy is identical to that of Shannon entropy when the BPA deﬁnes a probability measure. Numerical examples are illustrated to show the eﬃciency of Deng entropy.
© 2016 Elsevier Ltd. All rights reserved.

1. Introduction
How to measure the uncertainty has attracted much attention [1,2]. A lot of theories have been developed, for example, probability theory [3], fuzzy set theory [4], Dempster-Shafer evidence theory [5,6], rough sets [7], generalized evidence theory [8] and D numbers [9].
Dempster-Shafer theory evidence theory [5,6] is widely used in many applications such as decision making [10–13], supplier management [14–16], pattern recognition [17], risk evaluation [18,19] and so on [20]. However, some open issues are not well addressed. First, conﬂicting management should be taken into consideration when evidence highly conﬂicts with each other, since it may obtain the count-intuitive results [8,21]. Second, the assumption of evidence independent with each other is not satisﬁed in real application [22]. Third, how to generate the basic probability assignment (BPA) is still an open issue [23,24]. Finally, how to measure the uncertain degree of BPA is not yet solved, which is the aim of this paper. In Dempster-Shafer evidence theory, the uncertainty simultaneously contains nonspeciﬁcity and discord [25] which are coexisting in a basic probability assignment function (BPA). Several uncertainty measures [26], such as AU [27,28], AM [25], have been proposed to quantify such uncertainty in Dempster-Shafer theory. Some computing algorithms are also presented to obtain the uncertain degree [29]. What’s more, ﬁve axiomatic requirements have been further built in order to develop a justiﬁable measure. These ﬁve axiomatic requirements are range, probabilistic consistency, set consistency, additivity, subaddi-
E-mail address: ydeng@xjtu.edu.cn, prof.deng@hotmail.com
http://dx.doi.org/10.1016/j.chaos.2016.07.014 0960-0779/© 2016 Elsevier Ltd. All rights reserved.

tivity, respectively [30]. However, existing methods are not eﬃcient to measure uncertain degree of BPA [31–33].
Since ﬁrstly proposed by Clausius in 1865 for thermodynamics [34], various types of entropies are presented, such as information entropy [35], Tsallis entropy [36], nonadditive entropy [37]. Information entropy [35], derived from the Boltzmann-Gibbs (BG) entropy [38] in thermodynamics and statistical mechanics, has been an indicator to measure uncertainty which is associated with a probability density function (PDF). In this paper, a new entropy, named as Deng entropy, is proposed to handle the uncertain measure of BPA. Deng entropy can be seen as the generalized Shannon entropy. When the BPA is degenerated as probability distribution, Deng entropy is degenerated as Shannon entropy.
The paper is organized as follows. The preliminaries DempsterShafer evidence theory and entropy are brieﬂy introduced in Section 2. Section 3 presents Deng entropy and gives its important theoretical features. Some numerical examples are illustrated in Section 4 to show the eﬃciency of Deng entropy. Finally, this paper is concluded in Section 5.
2. Preliminaries
In this section, some preliminaries are brieﬂy introduced.
2.1. Dempster-Shafer evidence theory
Dempster-Shafer evidence theory has many advantages to handle uncertain information. Some basic concepts in D-S theory are introduced as follows [5,6].

550

Y. Deng / Chaos, Solitons and Fractals 91 (2016) 549–553

Fig. 1. A game of picking ball which can be handled by probability theory. (For interpretation of the references to colour in this ﬁgure legend, the reader is referred to the web version of this article.)

Fig. 2. A game of picking ball where probability theory is unable but D-S theory is able to handle. (For interpretation of the references to colour in this ﬁgure legend, the reader is referred to the web version of this article.)

Let X be a set of mutually exclusive and collectively exhaustive events, indicated by

X = {θ1, θ2, · · · , θi, · · · , θ|X|}

(1)

where set X is called a frame of discernment. The power set of X is indicated by 2X, namely

2X = {∅, {θ1}, · · · , {θ|X|}, {θ1, θ2}, · · · , {θ1, θ2, · · · , θi}, · · · , X} (2)

For a frame of discernment X = {θ1, θ2, · · · , θ|X|}, a mass func-
tion is a mapping m from 2X to [0, 1], formally deﬁned by [5,6]:

m : 2X → [0, 1]

(3)

which satisﬁes the following condition:

m(∅) = 0 and

m(A) = 1

(4)

A ∈ 2 X

A mass function is also called a basic probability assignment

(BPA). Assume there are two BPAs indicated by m1 and m2, the Dempster’s rule of combination is used to combine them as follows

[5,6] :

⎧

m (A )

=

⎨ 1
1 −
⎩0 ,

K

m1 (B )m2 (C )
B∩C=A

,

A = ∅; A = ∅.

(5)

with

K =

m1 (B )m2 (C )

(6)

B∩C=∅

Note that the Dempster’s rule of combination is only applicable to such two BPAs which satisfy the condition K < 1.
Here is an example to show the eﬃciency to model uncertainty of evidence theory. As shown in Fig. 1, there are two boxes. There are red balls in the left box, and green balls in the right box. The number of balls in each box is unknown. Now, a ball is picked randomly from these two boxes. We know that the left box is selected with a probability P1 = 0.6, and the right box is selected with a probability P2 = 0.4. Based on probability theory, it can be obtained that the probability of picking a red ball is 0.6, the proba-
bility of picking a green ball is 0.4, namely p(R) = 0.6, p(G) = 0.4.
Now, let us change the situation, as shown in Fig. 2. In the left box, there are still only red balls. But in the right box, there are not only red balls but also green balls. The exact number of the balls in each box is still unknown and the ratio of red balls with green balls is completely unknown. We know that the left box is selected with a probability P1 = 0.6, and the right box is selected with a probability P2 = 0.4. The question is what the probability that a red ball is picked. Obviously, due to the lack of adequate information, p(R) and p(G) cannot be obtained in this case. Facing the situation of inadequate information, probability

theory is incapable. However, if using D-S theory to analyze this
problem, we can obtain a BPA that m(R) = 0.6 and m(R, G) = 0.4.
In the framework of evidence theory, the uncertainty has been expressed in a reasonable way.

2.2. Existing entropy and open issue

The concept of entropy is derived from physics [34]. In thermodynamics and statistical mechanics, the entropy often refers to Boltzmann-Gibbs entropy [38]. According to Boltzmann’s H theorem, the Boltzmann-Gibbs (BG) entropy of an isolated system SBG is obtained in terms of the probabilities associated the distinct microscopic states available to the system given the macroscopic constraints, which has the following form

W

SBG = −k pi ln pi

(7)

i =1

where k is the Boltzmann constant, W is the amount of distinct

microscopic states available to the isolated system, pi is the proba-

bility of microscopic state i satisfying

W i =1

p i

=

1 .

Equal

probabili-

ties, i.e. ∀i, pi = 1/W, is a particular situation. In that situation, BG

entropy has the following form

SBG = k ln W

(8)

In information theory, Shannon entropy [35] is often used to measure the information volume of a system or a process, and quantify the expected value of the information contained in a message. Information entropy, denoted as H, has a similar form with BG entropy

N

H = − pi logb pi

(9)

i =1

where N is the amount of basic states in a state space, pi is the
W
probability of state i appears satisfying pi = 1, b is base of log-
i =1
arithm. When b = 2, the unit of information entropy is bit. If each
state equally appears, the quantity of H has this form

H = log2 N

(10)

In information theory, quantities of H play a central role as measures of information, choice and uncertainty. For example, the Shannon entropy of the game shown in Fig. 1 is H = 0.6 × log2 0.6 + 0.4 × log2 0.4 = 0.9710. But, it is worthy to notice that the uncertainty of this game shown in Fig. 2 cannot be calculated by using the Shannon entropy.
According to mentioned above, no matter the BG entropy or the information entropy, the quantity of entropy is always associated with the amount of states in a system. Especially, for the case of equal probabilities, the entropy or the uncertainty of a system is a

Y. Deng / Chaos, Solitons and Fractals 91 (2016) 549–553

551

Table 1 Some of existing uncertainty measures for mass functions.

Name

Formula

Dubois & Prade’s weighted Hartley entropy [39] Hohle’s confusion measure [40] Yager’s dissonance measure [41] Klir & Ramer’s discord [42] Klir & Parviz’s strife [43] George & Pal’s conﬂict measure [44]

IDP (m) = m(A) log2 |A|

A ⊆X

CH (m) = − m(A) log2 Bel(A)

A ⊆X

EY (m) = − m(A) log2 Pl(A)

A ⊆X

DKR (m) = −

m(A) log2

m (B

)

|A B| | B |

A ⊆X

B ⊆X

SKP (m) = −

m(A) log2

m (B

)

|A B| | A |

A ⊆X

B ⊆X

CGP (m) =

m (A )

m(B)[1 −

| A | A

B | B |

]

A ⊆X

B ⊆X

function of the quantity of states. Moreover, in that particular case, the entropy is the maximum.

3. Deng entropy

Beneﬁted from the Shannon entropy, the uncertainty measure of probability measures has a widely accepted solution. But for mass functions, it is still an open issue. In the last decades, various uncertainty measures have been proposed. Typical examples are shown in Table 1. But all of them are not entirely satisfactory. In this paper, a new uncertainty measure named as Deng entropy is proposed as follows

Ed(m) = −

m (A )

log2

m (A )
2|A| − 1

(11)

A ⊆X

where m is a mass function deﬁned on the frame of discernment X, and A is the focal element of m, |A| is the cardinality of A. As shown in the above deﬁnition, Deng entropy, formally, is similar with the classical Shannon entropy, but the belief for each focal
element A is divided by a term (2|A| − 1) which represents the
potential number of states in A (of course, the empty set is not included). Through a simple transformation, it is found that Deng entropy is actually a type of composite measures, as follows:

Ed (m) = m(A) log2(2|A| − 1) − m(A) log2 m(A)

(12)

A ⊆X

A ⊆X

where the term m(A) log2(2|A| − 1) could be interpreted as a
A ⊆X
measure of total nonspeciﬁcity in the mass function m, and the
term − m(A) log2 m(A) is the measure of discord of the mass
A ⊆X
function among various focal elements.
Different from traditional opinions, regarding the nonspeciﬁcity
in a set A, we have a distinct idea on this issue. For the conve-
nience of understanding, let us consider the following problem.

Example 3.1. Assume in a test there are 32 participants. The organizer has all the scores of these participants’ performance. If the organizer is only allowed to answer “Yes” or “No” to any questions, in order to know who IS(ARE) the top 1 student(s) who get(s) the highest score(s), how many times do we need ask at most?
Essentially, this is a problem about nonspeciﬁcity to ﬁnd the solution(s) among multiple alternatives. When there is only a top 1 participant who has the only highest score, the nonspeciﬁcity of the system is

t = log2 32 = 5.
It is easy to understand that we need ask at most ﬁve times to ﬁnd out the one who has the highest score, because there are 32 basic states in which each basic state stands for a possible result. However, if there are participants tied for ﬁrst, the nonspeciﬁcity of the new system is inevitable bigger than 5. By considering the most uncertain situation that there may be one top 1 participant,

or there may be two participants tied for ﬁrst, or three tied for ﬁrst, … , or 32 participants tied for ﬁrst, then we have
t = log2(232 − 1).
In this situation, due to the existence of cases of “tied for ﬁrst”, the number of basic states is not 32, but 232 − 1, so that the un-
certainty contained in the new system becomes log2(232 − 1).
This example shows that the kind of uncertainty, nonspeciﬁcity, could be bigger than the traditional one if the system is allowed to has multiple solutions.

4. Numerical examples

In the section, a lot of examples are given to show the effectiveness of Deng entropy.

Example 4.1. Assume there is a mass function m(a) = 1, the asso-
ciated Shannon entropy H and Deng entropy Ed are calculated as follows.

H(m) = 1 × log2 1 = 0

Ed (m)

=

1

× log2

1 21 −

1

=

0

Example 4.2. Given a frame of discernment X = {a, b, c}, for a mass function m(a) = m(b) = m(c) = 1/3, the associated Shannon
entropy H and Deng entropy Ed are

H(m)

=

−

1 3

×

log2

1 3

−

1 3

×

log2

1 3

−

1 3

×

log2

1 3

=

1 . 5850

Ed (m)

=

−

1 3

×

log2

1 / 3 21 − 1

−

1 3

×

log2

1 / 3 21 − 1

−

1 3

×

log2

1 / 3 21 − 1

= 1.5850

Clearly, Examples 4.1 and 4.2 have shown that the results of Shannon entropy and Deng entropy are identical when the belief is only assigned on single elements, which is proved by the probability consistency of Deng entropy.

Example 4.3. Given a frame of discernment X = {a, b, c}, for a mass function m1(a, b, c) = 1,

Ed (m1 )

=

−1

×

log2

1 23 −

1

=

2 . 8074

In comparison of Example 4.2, the entropy value of vacuous mass function m1 is bigger than that of the Bayesian mass function m shown in Example 4.2. The result is reasonable. Because the vacuous mass function represents the information is totally unknown for the system. But the Bayesian mass function shows that the probability is equally distributed for the system. Therefore, the Bayesian mass function has more information than the vacuous mass function, and its uncertainty should be smaller than that of the vacuous mass function.

552

Y. Deng / Chaos, Solitons and Fractals 91 (2016) 549–553

70

m1

60

m2

m3

50

m4

40

Deng entropy

30

20

10

0

0

5

10

15

20

25

30

35

40

N

Fig. 3. Deng entropy as a function of the size of frame of discernment in four types of mass functions.

For another mass function m2(a) = m2(b) = m2(c) = m2(a, b) = m2(a, c) = m2(b, c) = m2(a, b, c) = 1/7,

Ed (m2 )

=

−

1 7

×

log2

1 / 7 21 −1

−

1 7

×

log2

1 / 7 21 −1

−

1 7

×

log2

1 / 7 21 −1

−

1 7

×

log2

1 / 7 22 −1

−

1 7

×

log2

1 / 7 22 −1

−

1 7

×

log2

1 / 7 22 −1

−

1 7

×

log2

1 / 7 23 −1

= 3.8877

And for m3(A) =

2|A| −1 2|B| −1

,

A, B ⊆ X,

namely

m3(a) = m3(b) =

B ⊆X

m3 (c )

=

1 19

,

m3(a, b)

=

m3(b, c)

=

m3(a, c)

=

3 19

,

m3(a, b, c)

=

7 19

,

Ed (m3 )

=

−

1 19

log2

1 / 19 21 −1

−

1 19

log2

1 / 19 21 −1

−

1 19

log2

1 / 19 21 −1

−

3 19

log2

3 / 19 22 −1

−

3 19

log2

3 / 19 22 −1

−

3 19

log2

3 / 19 22 −1

−

7 19

log2

7 / 19 23 −1

= 4.2479

m3 is the mass function having the maximum Deng entropy for
the frame of discernment X = {a, b, c}, its uncertainty can also be
calculated by
log2 (2|B| − 1)
B ⊆X
= log2[(21 − 1) + (21 − 1) + (21 − 1) + (22 − 1) + (22 − 1) +(22 − 1) + (23 − 1)]
= log2(1 + 1 + 1 + 3 + 3 + 3 + 7)
= log2 19 = 4.2479

Example 4.4. Given a frame of discernment X = {a1, a2, · · · , aN},
let us consider four special cases of mass functions as follows.

• m1(A) =

2|A| −1 2|B| −1

,

A, B

⊆

X.

B ⊆X

• m2(A) = m2(B) and

m2(A) = 1, ∀A, B ⊆ X, A, B = ∅.

A ⊆X

• m3(X ) = 1.

• m4(a1 ) = m4(a2 ) = · · · = m4(aN ) = 1/N.

Their associated Deng entropies change with N, as shown in
Fig. 3. It is found again that m1 has the maximum uncertainty and the Bayesian mass function m4 has the minimum uncertainty among these four mass functions.

Example 4.5. Given a frame of discernment X with 15 elements which are denoted as element 1, element 2, etc. A mass function is shown as follows.
m({3, 4, 5}) = 0.05, m({6}) = 0.05, m(A) = 0.8, m(X ) = 0.1

Table 2 Deng entropy when A changes.

Cases

Deng entropy

A = {1} A = {1, 2} A = {1, 2, 3} A = {1, · · · , 4} A = {1, · · · , 5} A = {1, · · · , 6} A = {1, · · · , 7} A = {1, · · · , 8} A = {1, · · · , 9} A = {1, · · · , 10} A = {1, · · · , 11} A = {1, · · · , 12} A = {1, · · · , 13} A = {1, · · · , 14}

2.6623 3.9303 4.9082 5.7878 6.6256 7.4441 8.2352 9.0578 9.8600 10.6612 11.4617 12.2620 13.0622 13.8622

14

12

10

Deng entropy

8

6

4

2

0

2

4

6

8

10

12

14

Size of A

Fig. 4. Deng entropy as a function of the size of A.

14

Weighted Hartley entropy

12

Confusion

Dissonance

Discord

10

Strife

Conflict measure

8

Deng entropy

Uncertainty degree

6

4

2

0

0

2

4

6

8

10

12

14

Size of A

Fig. 5. Comparison between Deng entropy and other uncertainty measures by using Example 4.5.

Table 2 lists various Deng entropies when A changes, which is graphically shown in Fig. 4. The results shows that the entropy of m increases monotonously with the rise of the size of subset A. It is rational that the entropy increases when the uncertainty involving a mass function increases.
What’s more, Fig. 5 shows the uncertainty degree of mass function m by using other different uncertainty measures including Dubois & Prade’s weighted Hartley entropy [39], Hohle’s confusion measure [40], Yager’s dissonance measure [41], Klir & Ramer’s discord [42], Klir & Parviz’s strife [43], George & Pal’s conﬂict measure [44]. From Fig. 5, it shows that only Deng entropy and weighted Hartley entropy increase monotonously with the rise of the size of

Y. Deng / Chaos, Solitons and Fractals 91 (2016) 549–553

553

set A. In contrast, the values derived by others uncertainty measures are declining or changing irregularly as the size of A increases, which is obviously irrational. Besides, in comparison of Deng entropy and weighted Hartley entropy, according to their formulas, it is found that weighted Hartley entropy can not reduce to the Shannon entropy when the mass function deﬁnes a probability measure. Therefore, Deng entropy is the only reasonable measure among these given uncertainty measures.
5. Conclusion
Shannon entropy can eﬃciently measure the uncertain degree with probability distribution. However, how to measure uncertain degree with basic probability assignment is still an open issue. The main contribution of this paper is that a new entropy, named as Deng entropy, is presented. Deng entropy is the generalization of Shannon entropy. When the BPA is degenerated as a probability distribution, Deng entropy is degenerated as Shannon entropy. Numerical examples are illustrated to show the eﬃciency of Deng entropy. Some properties of Deng entropy are discussed. The new entropy provides a promising way to measure uncertain degree and to handle more uncertain information.
Acknowledgments
The author greatly appreciates the reviews’ valuable suggestions and the editor’s encouragement. The author greatly appreciates Professor Shan Zhong, the China academician of the Academy of Engineering, for his encouragement to do this research. The author greatly appreciates Professor Yugeng Xi in Shanghai Jiao Tong University, Professor Sankaran Mahadevan in Vanderbilt University, Professor Rehan Sadiq in The University of British Columbia for their support to this work. The author’s Ph.D students in Shanghai Jiao Tong University, Peida Xu and Xiaoyan Su, discussed the uncertain measure. The author’s Ph.D students in Southwest University, Xinyang Deng and Bingyi Kang, graduate student in Southwest University, Liguo Fei discussed some properties of the entropy. The author greatly appreciates the continuous funding for the last ten years. This work is partially supported by National Natural Science Foundation of China, Grant Nos. 30400067, 60874105, 61174022 and 61573290, the Chenxing Scholarship Youth Found of Shanghai Jiao Tong University, Grant No. T241460612, Program for New Century Excellent Talents in University, Grant No. NCET08-0345, Shanghai Rising-Star Program Grant No.09QA1402900, Chongqing Natural Science Foundation for distinguished scientist, Grant No. CSCT, 2010BA2003.
References
[1] Klir GJ. Uncertainty and information: foundations of generalized information theory. John Wiley & Sons; 2005.
[2] Mahadevan S, Haldar A. Probability, reliability and statistical method in engineering design. John Wiley & Sons; 2000.
[3] Feller W. An introduction to probability theory and its applications, 2. John Wiley & Sons; 2008.
[4] Zadeh LA. Fuzzy sets. Inf Control 1965;8(3):338–53. [5] Dempster AP. Upper and lower probabilities induced by a multivalued map-
ping. Ann Math Stat 1967;38(2):325–39. [6] Shafer G. A mathematical theory of evidence. Princeton: Princeton University
Press; 1976. [7] Pawlak Z. Rough sets. Int J Comput Inf Sci 1982;11(5):341–56. [8] Deng Y. Generalized evidence theory. Appl Intell 2015;43(3):530–43. [9] Deng Y. D numbers: theory and applications. Int J Comput Inf Sci
2012;9(9):2421–8 .

[10] Yang J-B, Xu D-L. Evidential reasoning rule for evidence combination. Artif Intell 2013;205:1–29.
[11] Utkin LV. A new ranking procedure by incomplete pairwise comparisons using preference subsets. Intell Data Analy 2009;13(2):229–41.
[12] Fan G, Zhong D, Yan F, Yue P. A hybrid fuzzy evaluation method for curtain grouting eﬃciency assessment based on an AHP method extended by D numbers. Expert Syst Appl 2016;44(1):289–303.
[13] Fu C, Yang J-B, Yang S-L. A group evidential reasoning approach based on expert reliability. Eur J Oper Res 2015;246(3):886–93.
[14] Deng X, Hu Y, Deng Y, Mahadevan S. Supplier selection using AHP methodology extended by D numbers. Expert Syst Appl 2014;41(1):156–67.
[15] Deng Y, Chan F. A new fuzzy dempster MCDM method and its application in supplier selection. Expert Syst Appl 2011;38(8):9854–61.
[16] Ma J, Liu W, Miller P, Zhou H. An evidential fusion approach for gender proﬁling. Inf Sci 2016;333:10–20.
[17] Huynh V-N, Nguyen TT, Le CA. Adaptively entropy-based weighting classiﬁers in combination using Dempster Shafer theory for word sense disambiguation. Comput Speech Lang 2010;24(3):461–73.
[18] Kabir G, Tesfamariam S, Francisque A, Sadiq R. Evaluating risk of water mains failure using a Bayesian belief network model. Eur J Oper Res 2015;240(1):220–34 .
[19] Liu HC, You JX, Fan XJ, Lin QL. Failure mode and effects analysis using D numbers and grey relational projection method. Expert Syst Appl 2014;41(10):4670–9 .
[20] Cuzzolin F. A geometric approach to the theory of evidence. Syst Man Cybern Part C 2008;38(4):522–34.
[21] Schubert J. Conﬂict management in Dempster-Shafer theory using the degree of falsity. Int J Approx Reason 2011;52(3):449–60.
[22] Su X, Mahadevan S, Han W, Deng Y. Combining dependent bodies of evidence. Appl Intell 2016;44:634–44.
[23] Xu P, Su X, Mahadevan S, Li C, Deng Y. A non-parametric method to determine basic probability assignment for classiﬁcation problems. Appl Intell 2014;41(3):681–93.
[24] Jiang W, Zhan J, Zhou D, Li X. A method to determine generalized basic probability assignment in the open world. Math Probl Eng 2016;Article ID 373142:11pages .
[25] Jousselme A-L, Liu C, Grenier D, Bosse E. Measuring ambiguity in the evidence theory. IEEE Trans Syst Man Cybern Part A 2006;36(5):890–903.
[26] Yang Y, Han D. A new distance-based total uncertainty measure in the theory of belief functions. Knowl Based Syst 2016;94:114–23.
[27] Maeda Y, Nguyen HT, Ichihashi H. Maximum entropy algorithms for uncertainty measures. Int J Uncertain Fuzz Knowl Based Syst 1993;1(01):69–93.
[28] Harmanec D, Klir GJ. Measuring total uncertainty in dempster-shafer theory: a novel approach. Int J Gen Syst 1994;22(4):405–19.
[29] Huynh V-N, Nakamori Y. Notes on “Reducing Algorithm Complexity for Computing an Aggregate Uncertainty Measure”. IEEE Trans Syst Man Cybern Part A 2010;40(1):205–9 .
[30] Klir GJ, Wierman MJ. Uncertainty-based information: elements of generalized information theory. Springer; 1999.
[31] Deng X, Deng Y. On the axiomatic requirement of range to measure uncertainty. Physica A 2014;406:163–8.
[32] Yuan K, Xiao F, Fei L, Kang B, Deng Y. Modeling sensor reliability in fault diagnosis based on evidence theory. Sensors 2016;16(1):113. doi:10.3390/ s16010113 .
[33] Jiang W, Wei B, Xie C, Zhou D. An evidential sensor fusion method in fault diagnosis. Adv Mech Eng 2016;8(3):1–7.
[34] Clausius R. The mechanical theory of heat: with its applications to the steam-engine and to the physical properties of bodies. J. van Voorst; 1867.
[35] Shannon CE. A mathematical theory of communication. ACM SIGMOBILE Mob Comput Commun Rev 2001;5(1):3–55.
[36] Tsallis C. Possible generalization of Boltzmann-Gibbs statistics. J Stat Phys 1988;52(1–2):479–87.
[37] Tsallis C. Nonadditive entropy: the concept and its use. Eur Phys J A 2009;40(3):257–66.
[38] Lebowitz JL. Boltzmann’s entropy and time’s arrow. Phys Today 1993;46. 32–32 [39] Dubois D, Prade H. A note on measures of speciﬁcity for fuzzy sets. Int J Gen
Syst 1985;10(4):279–83. [40] Höhle U. Entropy with respect to plausibility measures. In: Proceedings of the
12th IEEE international symposium on multiple-valued logic; 1982. p. 167–9. [41] Yager RR. Entropy and speciﬁcity in a mathematical theory of evidence. Int J
Gen Syst 1983;9(4):249–60. [42] Klir GJ, Ramer A. Uncertainty in the Dempster-Shafer theory: a critical re-ex-
amination. Int J Gen Syst 1990;18(2):155–66. [43] Klir GJ, Parviz B. A note on the measure of discord. In: Proceedings of the
Eighth international conference on Uncertainty in artiﬁcial intelligence. Morgan Kaufmann Publishers Inc.; 1992. p. 138–41. [44] George T, Pal NR. Quantiﬁcation of conﬂict in Dempster-Shafer framework: a new approach. Int J Gen Syst 1996;24(4):407–23.

