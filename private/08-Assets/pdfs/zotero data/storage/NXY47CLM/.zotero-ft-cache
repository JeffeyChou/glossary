
Processing math: 68%
JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page. Skip to main content Skip to article
Elsevier logo ScienceDirect

    Journals & Books 

    Search 

周杰锋 UESTC
周U

    View  PDF
    Download full issue 

Outline

    Highlights
    Abstract
    Keywords
    1. Introduction
    2. Shannon's entropy of PMFs of discrete random variables
    3. Basic definitions of the DS belief functions theory
    4. Required properties of entropy of BPAs in the DS theory
    5. Previous definitions of entropy of BPAs in the DS theory
    6. A new definition of entropy for DS theory
    7. Additional properties of H ( m )
    8. Summary and conclusion
    Acknowledgements
    References 

Cited By (118)
Tables (1)

    Table 1 

Elsevier
International Journal of Approximate Reasoning
Volume 92 , January 2018, Pages 49-65
International Journal of Approximate Reasoning
A new definition of entropy of belief functions in the Dempster–Shafer theory ☆
Author links open overlay panel Radim Jiroušek a , Prakash P. Shenoy b
Show more
Add to Mendeley
Share
Cite
https://doi.org/10.1016/j.ijar.2017.10.010 Get rights and content
Under an Elsevier user license
open archive
Highlights

    •

    We state six desired properties that should be satisfied by an entropy function in the Dempster–Shafer theory.
    •

    Three of the six desired properties are different from the five properties defined by Klir and Wierman.
    •

    We show that all previously defined entropy functions do not meet all six desired properties.
    •

    We prove that our new definition of entropy satisfies all six desired properties.
    •

    We discuss also the subadditivity property and show that our definition does not satisfy it.

Abstract

We propose a new definition of entropy of basic probability assignments (BPAs) in the Dempster–Shafer (DS) theory of belief functions, which is interpreted as a measure of total uncertainty in the BPA. Our definition is different from those proposed by Höhle, Smets, Yager, Nguyen, Dubois–Prade, Lamata–Moral, Klir–Ramer, Klir–Parviz, Pal et al., Maeda–Ichihashi, Harmanec–Klir, Abellán–Moral, Jousselme et al., Pouly et al., and Deng. We state a list of six desired properties of entropy for DS belief functions theory, four of which are motivated by Shannon's definition of entropy of probability functions , and the remaining two are requirements that adapt this measure to the philosophy of the DS theory. Three of our six desired properties are different from the five properties proposed by Klir and Wierman. We demonstrate that our definition satisfies all six properties in our list, whereas none of the existing definitions do. Our new definition has two components. The first component is Shannon's entropy of an equivalent probability mass function obtained using the plausibility transform, which constitutes the conflict measure of entropy. The second component is Dubois–Prade's definition of entropy of basic probability assignments in the DS theory, which constitutes the non-specificity measure of entropy. Our new definition is the sum of these two components. Our definition does not satisfy the subadditivity property. Whether there exists a definition that satisfies our six properties plus subadditivity remains an open question.

    Previous article in issue
    Next article in issue 

Keywords
Dempster–Shafer theory of belief functions
Plausibility transform of a belief function
Dempster–Shafer theory semantics
Dempster's rule of combination
Maximum entropy property
1. Introduction

The main goal of this paper is to propose a new definition of entropy of a basic probability assignment (BPA) in the Dempster–Shafer (DS) theory of belief functions [9] , [42] . Since 1982, when Höhle [20] gave a first definition of entropy of a BPA in the DS theory, there have been numerous definitions of entropies of a BPA. So an obvious question is: Why do we need another definition of entropy of a BPA? In the remainder of this section, we attempt to answer this question.

We follow an axiomatic approach to defining entropy of a BPA. First, we state a list of six desirable properties , and then we provide a definition that satisfies the six properties. The axiomatic approach to defining entropy of a BPA is not new. Klir and Wierman [26] state five properties that they claim are essential in defining entropy of a BPA. However, as we will argue, Klir–Wierman's properties are unsatisfactory to us. Our set of six properties is designed to address some of the shortcomings of Klir–Wierman's properties. Abellán and Moral [4] also propose additional properties. Some of these are also discussed in this paper.

First, there are several theories of belief functions in the literature. In this paper, we are concerned only with the Dempster–Shafer theory, which has as its centerpiece, Dempster's combination rule as the rule for aggregating evidence. For example, in the imprecise probability community, belief functions are regarded as encoding a set of probability mass functions (PMFs), whose lower envelope constitutes a belief function. Such a set of PMFs is called a credal set . However, as we will argue in Section 4 , credal set semantics of belief functions are incompatible with Dempster's combination rule [43] , [44] , [45] , [16] . Our goal is to define entropy of a BPA in the DS theory. Therefore, the first property we propose, called “consistency with DS theory semantics,” is that a definition of entropy of a BPA in the DS theory should be based on interpretations of the BPA that are compatible with the basic tenets of DS theory, namely, Dempster's combination rule.

One method for defining entropy of a BPA m is to first transform the BPA to a corresponding PMF P m , and then use Shannon's entropy of P m as the entropy of m (see, e.g., [31] , [17] , [4] , [22] , [38] ). However, there are many ways to make such a transformation. Voorbraak [55] , and Cobb and Shenoy [6] , argue that any transform of BPA m in the DS theory should be consistent with Dempster's combination rule in the sense that P m 1 ⊕ m 2 = P m 1 ⊗ P m 2 , where ⊕ denotes Dempster's combination rule, and ⊗ denotes Bayesian combination rule, i.e., pointwise multiplication followed by normalization. They propose a plausibility transform that satisfies this consistency requirement, and it can be shown that the plausibility transform is the only transform that satisfies such a consistency requirement. Our consistency with DS theory semantics property entails that if such a transform is used to define entropy of a BPA, then the transform must be the plausibility transform. This is to ensure that any definition of entropy of a BPA is relevant for the DS theory of belief functions.

Klir–Wierman's set of five properties, and our proposed set of six properties, include an additivity property that states that if m X and m Y are distinct BPAs for distinct variables X and Y , then entropy of m X ⊕ m Y should be the sum of the entropies of m X and m Y . Unfortunately, this additivity property is extremely weak, and is satisfied by almost all definitions that have been proposed in the literature. Our consistency with DS semantics property helps to bolster the additivity property.

Second, the DS theory is considered more expressive than probability theory in representing ignorance. In probability theory, both vacuous knowledge of variable X with state space Ω X , and knowledge that all states in Ω X are equally likely are represented by the equally-likely PMF of X . In DS theory, we can represent vacuous knowledge of X by the vacuous BPA for X , and we can represent the knowledge that all states are equally likely by the equally-likely Bayesian BPA for X . Ellsberg [14] demonstrates that when offered a choice, many prefer to bet on the outcome of an urn with 50 red and 50 blue balls rather than on one with 100 total balls but for which the number of blue or red balls is unknown. This phenomenon is called Ellsberg paradox , as Savage's subjective expected utility theory [41] is unable to account for this human behavior. Two of Klir–Wierman's properties (called “set consistency” and “range”) entail that the entropy of the vacuous BPA for X is equal to the entropy of the equally-likely Bayesian BPA for X . In our opinion, this is unacceptable. Clearly, there is greater uncertainty in a vacuous BPA than in an equally-likely Bayesian BPA, a fact demonstrated by Ellsberg paradox. Therefore, instead of these two properties, we formulate a “maximum entropy” property that states that entropy of a BPA m for X is less than or equal to the entropy of the vacuous BPA for X , with equality if and only if m is the vacuous BPA for X . Abellán and Moral [4] were the earliest to propose such a maximum entropy property.

An outline of the remainder of the paper is as follows. In Section 2 , we briefly review Shannon's definition of entropy for PMFs of discrete random variables , and its properties. In Section 3 , we review the basic definitions in the DS belief functions theory. In Section 4 , we propose six properties that an entropy function for BPA should satisfy. We compare our properties with those proposed by Klir and Wierman [26] , and also with a set monotonicity property proposed by Abellán and Masegosa [3] . In Section 5 , we discuss the various definitions that have been proposed in the literature, and how they compare vis-a-vis our list of six properties. In Section 6 , we propose a new definition of entropy for DS theory, and show that it satisfies all six properties proposed in Section 4 . In Section 7 , we discuss some additional properties of our definition. Finally, in Section 8 , we summarize our findings, and conclude with some open questions.
2. Shannon's entropy of PMFs of discrete random variables

In this section we briefly review Shannon's definition of entropy of PMFs of discrete random variables , and its properties. Most of the material in this section is taken from [47] , [30] .

Information content. Suppose X is a discrete random variable , with state space Ω X , and PMF P X . Consider a state x ∈ Ω X such that P X ( x ) > 0 . What is the information content of this state? Shannon [47] defines the information content of state x ∈ Ω X as follows: (1) I ( x ) = log 2 ⁡ ( 1 P X ( x ) ) .

Information content has units of bits . Intuitively, the information content of a state is inversely proportional to its probability. Observing a state with probability one has no information content (0 bits). Notice that I ( x ) ≥ 0 , and I ( x ) = 0 if and only if P X ( x ) = 1 .

Although we have used logarithm to the base 2, we could use any base (e.g., e , or 10), but this will change the units. Henceforth, we will simply write log for log 2 .

Shannon's entropy . Suppose X is a random variable with PMF P X . The entropy of P X is the expected information content of the possible states of X : (2) H s ( P X ) = ∑ x ∈ Ω X P X ( x ) I ( x ) = ∑ x ∈ Ω X P X ( x ) log ⁡ ( 1 P X ( x ) ) .

Like information content, entropy is measured in units of bits. One can interpret entropy H s ( P X ) as a measure of uncertainty in the PMF P X ( x ) . If P X ( x ) = 0 , we follow the convention that P X ( x ) log ⁡ ( 1 / P X ( x ) ) = 0 as lim θ → 0 + ⁡ θ log ⁡ ( 1 / θ ) = 0 .

Suppose Y is another random variable, and suppose that the joint PMF of X and Y is P X , Y with P X and P Y as the marginal PMFs of X and Y , respectively. If we observe Y = a such that P Y ( a ) > 0 , then the posterior PMF of X is P X | a (where P X | a ( x ) = P X , Y ( x , a ) / P Y ( a ) ), and the respective posterior entropy is H s ( P X | a ) .

From our viewpoint, the following properties of Shannon's entropy function for PMFs are the most important ones:

    1.

    H s ( P X ) ≥ 0 , with equality if and only if there exists x ∈ Ω X such that P X ( x ) = 1 .
    2.

    H s ( P X ) ≤ log ⁡ ( | Ω X | ) , with equality if and only if P X ( x ) = 1 | Ω X | for all x ∈ Ω X . | Ω X | denotes the cardinality (i.e., number of elements) of set Ω X .
    3.

    The entropy of P X does not depend on the labels attached to the states of X , only on their probabilities. This is in contrast with, e.g., variance of X , which is defined only for real-valued random variables. Thus, for a real-valued discrete random variable X , and Y = 10 X , it is obvious that H s ( P Y ) = H s ( P X ) , whereas V ( P Y ) = 100 V ( P X ) .
    4.

    Shannon [47] derives the expression for entropy of X axiomatically using four axioms as follows.
        (a)

        Axiom 0 ( Existence ): H ( X ) exists.
        (b)

        Axiom 1 ( Continuity ): H ( X ) should be a continuous function of P X ( x ) for x ∈ Ω X .
        (c)

        Axiom 2 ( Monotonicity ): If we have an equally likely PMF, then H ( X ) should be a monotonically increasing function of | Ω X | .
        (d)

        Axiom 3 ( Compound distributions ): If a PMF is factored into two PMFs, then its entropy should be the sum of entropies of its factors, e.g., if P X , Y ( x , y ) = P X ( x ) P Y | x ( y ) , then H ( P X , Y ) = H ( P X ) + ∑ x ∈ Ω X P X ( x ) H ( P Y | x ) .

    Shannon [47] proves that the only function H s that satisfies Axioms 0–3 is of the form H s ( P X ) = K ∑ x ∈ Ω X P X ( x ) log ⁡ ( 1 P X ( x ) ) , where K is a constant depending on the choice of units of measurement .

Suppose X and Y are discrete random variables with joint PMF P X , Y . Analogous to the one-dimensional case, the joint entropy of P X , Y is: (3) H s ( P X , Y ) = ∑ x ∈ Ω X ∑ y ∈ Ω Y P X , Y ( x , y ) log ⁡ ( 1 P X , Y ( x , y ) ) .

Let P Y | X : Ω { X , Y } → [ 0 , 1 ] be a function such that P Y | X ( x , y ) = P Y | x ( y ) for all ( x , y ) ∈ Ω { X , Y } . Though P Y | X is called a conditional PMF, it is not a PMF. It is a collection of conditional PMFs, one for each x ∈ Ω X . If we combine P X and P Y | X using pointwise multiplication followed by normalization, an operation that we denote by ⊗, then we obtain P X , Y , i.e., P X , Y = P X ⊗ P Y | X , i.e., P X , Y ( x , y ) = P X ( x ) P Y | X ( x , y ) = P X ( x ) P Y | x ( y ) for all ( x , y ) ∈ Ω { X , Y } . As P X and P Y | x are PMFs, there is no need for normalization (or the normalization constant is 1).

Shannon defined the entropy of P Y | X as follows: (4) H s ( P Y | X ) = ∑ x ∈ Ω X P X ( x ) H s ( P Y | x ) . We call H s ( P Y | X ) the conditional entropy of Y given X .

It follows from Axiom 3 that (5) H s ( P X , Y ) = H s ( P X ⊗ P Y | X ) = H s ( P X ) + H s ( P Y | X ) . We call H s ( P X ) the marginal entropy of X , and Eq. (5) is the compound distribution axiom underlying Shannon's entropy expressed in terms of marginal and conditional entropies. Eq. (5) is also called the chain rule of entropy.

If X and Y are independent with respect to P X , Y , i.e., P Y | x ( y ) = P Y ( y ) for all ( x , y ) ∈ Ω { X , Y } such that P X ( x ) > 0 , then it follows from Eq. (4) that H s ( P Y | X ) = H s ( P Y ) . Thus, if X and Y are independent with respect to P X , Y , then H s ( P X , Y ) = H s ( P X ) + H s ( P Y ) .

Suppose P X and P Y are the marginal PMFs obtained from the joint PMF P X , Y . Then, it can be shown that (6) H s ( P X , Y ) ≤ H s ( P X ) + H s ( P Y ) , with equality if and only if X and Y are independent with respect to P X , Y . The inequality in Eq. (6) is called subadditivity in the literature (see, e.g., [13] ).
3. Basic definitions of the DS belief functions theory

In this section we review the basic definitions in the DS belief functions theory. Like the various uncertainty theories, DS belief functions theory includes functional representations of uncertain knowledge, and operations for making inferences from such knowledge.

Basic probability assignment . Suppose X is a random variable with state space Ω X . Let 2 Ω X denote the set of all non-empty subsets of Ω X . A basic probability assignment (BPA) m for X is a function m : 2 Ω X → [ 0 , 1 ] such that (7) ∑ a ∈ 2 Ω X m ( a ) = 1 . The subsets a ∈ 2 Ω X (recall that we exclude the empty set from 2 Ω X ) such that m ( a ) > 0 are called focal elements of m . An example of a BPA for X is the vacuous BPA for X , denoted by ι X , such that ι X ( Ω X ) = 1 . We say m is deterministic (or categorical ) if m has a single focal element (with probability 1). Thus, the vacuous BPA for X is deterministic with focal element Ω X . If all focal elements of m are singleton subsets of Ω X , then we say m is Bayesian . In this case, m is equivalent to the PMF P for X such that P ( x ) = m ( { x } ) for each x ∈ Ω X . Let m u denote the Bayesian BPA with uniform probabilities, i.e., m u ( { x } ) = 1 | Ω X | for all x ∈ Ω X . If Ω X is a focal element of m , then we say m is non-dogmatic , and dogmatic otherwise. Thus, a Bayesian BPA is dogmatic.

Plausibility function. The plausibility function P l m corresponding to BPA m is defined as follows: (8) P l m ( a ) = ∑ b ∈ 2 Ω X : b ∩ a ≠ ∅ m ( b ) for all a ∈ 2 Ω X . For an example, suppose Ω X = { x , x ¯ } . Then, the values of the plausibility function P l ι X corresponding to BPA ι X , are identically one for all three subsets in 2 Ω X .

Belief function. The belief function B e l m corresponding to BPA m is defined as follows: (9) B e l m ( a ) = ∑ b ∈ 2 Ω X : b ⊆ a m ( b ) for all a ∈ 2 Ω X . For the example above with Ω X = { x , x ¯ } , the belief function B e l ι X corresponding to BPA ι X is given by B e l ι X ( { x } ) = 0 , B e l ι X ( { x ¯ } ) = 0 , and B e l ι X ( Ω X ) = 1 .

Commonality function. The commonality function Q m corresponding to BPA m is defined as follows: (10) Q m ( a ) = ∑ b ∈ 2 Ω X : b ⊇ a m ( b ) for all a ∈ 2 Ω X . For the example above with Ω X = { x , x ¯ } , the commonality function Q ι X corresponding to BPA ι X is given by Q ι X ( { x } ) = 1 , Q ι X ( { x ¯ } ) = 1 , and Q ι X ( Ω X ) = 1 . If m is non-dogmatic, then Q m ( a ) > 0 for all a ∈ 2 Ω X . Notice also that for singleton subsets a ∈ 2 Ω X , Q m ( a ) = P l m ( a ) . This is because for singleton subsets a , the set of all subsets that have non-empty intersection with a coincides with the set of all supersets of a . Finally, Q m is a normalized function in the sense that: (11) ∑ a ∈ 2 Ω X ( − 1 ) | a | Q m ( a ) = ∑ b ∈ 2 Ω X m ( b ) = 1 .

All four representations—BPA, belief, plausibility, and commonality—are bearers of exactly the same information. Given any one, we can transform it to another [42] .

Next, we describe the two main operations for making inferences.

Dempster's combination rule. In the DS theory, we can combine two BPAs m 1 and m 2 representing distinct pieces of evidence by Dempster's rule [9] and obtain the BPA m 1 ⊕ m 2 , which represents the combined evidence. In this paper, it is sufficient to define Dempster's rule for BPAs for a single variable, and for BPAs for distinct variables.

Suppose m 1 and m 2 are two BPAs for X . Then, (12) ( m 1 ⊕ m 2 ) ( a ) = K − 1 ∑ b 1 , b 2 ∈ 2 Ω X : b 1 ∩ b 2 = a m 1 ( b 1 ) m 2 ( b 2 ) , for all a ∈ 2 Ω X , where K is a normalization constant given by (13) K = 1 − ∑ b 1 , b 2 ∈ 2 Ω X : b 1 ∩ b 2 = ∅ m 1 ( b 1 ) m 2 ( b 2 ) . The definition of Dempster's rule assumes that the normalization constant K is non-zero. If K = 0 , then the two BPAs m 1 and m 2 are said to be in total conflict and cannot be combined. If K = 1 , we say m 1 and m 2 are non-conflicting .

Dempster's rule can also be described in terms of commonality functions [42] . Suppose Q m 1 and Q m 2 are commonality functions corresponding to BPAs m 1 and m 2 , respectively. The commonality function Q m 1 ⊕ m 2 corresponding to BPA m 1 ⊕ m 2 is as follows: (14) Q m 1 ⊕ m 2 ( a ) = K − 1 Q m 1 ( a ) Q m 2 ( a ) , for all a ∈ 2 Ω X , where the normalization constant K is as follows: (15) K = ∑ a ∈ 2 Ω X ( − 1 ) | a | + 1 Q m 1 ( a ) Q m 2 ( a ) . It is shown in [42] that the normalization constant K in Eq. (15) is exactly the same as in Eq. (13) . So we see that in terms of commonality functions, Dempster's rule is pointwise multiplication of commonality functions followed by normalization.

Suppose that m X and m Y are two BPAs for X and Y , respectively. In this case, m X ⊕ m Y is a BPA for { X , Y } such that each of its focal element is a Cartesian product of a focal element of m X and a focal element of m Y . Formally, (16) ( m X ⊕ m Y ) ( a × b ) = m X ( a ) m Y ( b ) , for all a × b ∈ 2 Ω { X , Y } . Notice that in this case there is no need for normalization as there is no mass on the empty set, i.e., m X and m Y are always non-conflicting.

Marginalization . Marginalization in DS theory is addition of values of BPAs. To define marginalization formally, we first need to define projection of states, and then projection of subset of states.

Projection of states simply means dropping extra coordinates; for example, if ( x , y ) is a state of { X , Y } , then the projection of ( x , y ) to X , denoted by ( x , y ) ↓ X , is simply x , which is a state of X .

Projection of subsets of states is achieved by projecting every state in the subset. Suppose b ∈ 2 Ω { X , Y } . Then b ↓ X = { x ∈ Ω X : ( x , y ) ∈ b  for some  y ∈ Ω Y } . Notice that b ↓ X ∈ 2 Ω X .

Suppose m is a BPA for { X , Y } . Then, the marginal of m for X , denoted by m ↓ X , is a BPA for X such that for each a ∈ 2 Ω X , (17) m ↓ X ( a ) = ∑ b ∈ 2 Ω { X , Y } : b ↓ X = a m ( b ) .

In Eq. (16) , if we compute the marginals of the joint belief function m X ⊕ m Y for X and Y , then we obtain the original BPAs m X and m Y , respectively. Klir and Wierman [26] use the terminology: marginals m ↓ X and m ↓ Y are noninteractive if m = m ↓ X ⊕ m ↓ Y .

This completes our brief review of the DS belief function theory. For further details, we refer the reader to [42] .
4. Required properties of entropy of BPAs in the DS theory

In this section, we propose six basic properties that an entropy function for BPAs in the DS theory should satisfy, and compare them with those proposed by Klir and Wierman [26] for the same purposes. As a prelude to our first property, called consistency with DS theory semantics , we give some examples of interpretations of a BPA m that are inconsistent with DS theory semantics.

Credal set semantics of a BPA. For each BPA m for X , there exists a set P m of PMFs for X that is defined as follows [16] . Let P denote the set of all PMFs for X . Then, (18) P m = { P ∈ P : ∑ x ∈ a P ( x ) ≥ B e l m ( a ) = ∑ b ⊆ a m ( b )  for all  a ∈ 2 Ω X } . Thus, a BPA m can be interpreted as an encoding of a set of PMFs as described in Eq. (18) . If m = ι X , then P ι X includes the set of all PMFs for X . If m is a Bayesian BPA for X , then P m includes a single PMF P X corresponding to the Bayesian BPA m .

P m is referred to as a credal set corresponding to m (see, e.g., [56] ). Notice that P m is yet another equivalent representation of m , like B e l m , P l m , and Q m . Given P m , we can recover the other representations. As already mentioned in Section 1 , this interpretation of a BPA function is incompatible with Dempster's combination rule [43] , [44] , [45] , [16] , which is also illustrated in the following example.

Example 1

Consider a BPA m 1 for X with state space Ω X = { x 1 , x 2 , x 3 } as follows: m 1 ( { x 1 } ) = 0.5 , m 1 ( Ω X ) = 0.5 . With the credal set semantics of a BPA function, m 1 corresponds to a set of PMFs P m 1 = { P ∈ P : P ( x 1 ) ≥ 0.5 } , where P denotes the set of all PMFs for X . Now suppose we get a distinct piece of evidence m 2 for X such that m 2 ( { x 2 } ) = 0.5 , m 2 ( Ω X ) = 0.5 . m 2 corresponds to P m 2 = { P ∈ P : P ( x 2 ) ≥ 0.5 } . The only PMF that is in both P m 1 and P m 2 is P ∈ P such that P ( x 1 ) = P ( x 2 ) = 0.5 , and P ( x 3 ) = 0 . Notice that if we use Dempster's rule to combine m 1 and m 2 , we have: ( m 1 ⊕ m 2 ) ( { x 1 } ) = 1 3 , ( m 1 ⊕ m 2 ) ( { x 2 } ) = 1 3 , and ( m 1 ⊕ m 2 ) ( Ω X ) = 1 3 . The set of PMFs P m 1 ⊕ m 2 = { P ∈ P : P ( x 1 ) ≥ 1 3 , P ( x 2 ) ≥ 1 3 } is not the same as P m 1 ∩ P m 2 . Thus, credal set semantics of belief functions are incompatible with Dempster's combination rule.

Fagin and Halpern [15] propose another rule for updating beliefs, which is referred to as the Fagin–Halpern combination rule. If we start with a set of PMFs characterized by BPA m for X , and we observe some event b ⊂ Ω X , then one possible updating rule is to condition each PMF in the set P m on event b , and then find a BPA m ′ that corresponds to the lower envelope of the revised set of PMFs. The Fagin–Halpern rule [15] does precisely this, and is different from Dempster's rule of conditioning, which is a special case of Dempster's combination rule.

Transforming a BPA to a PMF. Given a BPA m for X in the DS theory, there are many ways to transform m to a corresponding PMF P m for X [8] , [7] , [46] . The main transforms used in the literature are the pignistic transform [12] , [49] , the maximum entropy credal set transform [31] , [17] , and the plausibility transform [55] , [6] . However, only the plausibility transform , is consistent with m in the DS theory in the sense that P m 1 ⊕ m 2 = P m 1 ⊗ P m 2 , where, as mentioned in Section 2 , ⊗ is the combination rule in probability theory, and ⊕ is Dempster's combination rule in DS theory [55] , [6] . Thus, if a probability transform is used to define entropy of m , then we argue that it must be the plausibility transform as it is the only one that is consistent with Dempster's combination rule.

First, let's define B e t P m . Suppose m is a BPA for X . Then B e t P m is a PMF for X defined as follows: (19) B e t P m ( x ) = ∑ a ∈ 2 Ω X : x ∈ a m ( a ) | a | for all x ∈ Ω X . It is easy to verify that B e t P m is a PMF. It is argued in [6] that B e t P m is an inappropriate probabilistic representation of m in the DS theory. The following example provides one reason why B e t P m is incompatible with Dempster's combination rule.

Example 2

This example is taken from [50] . Consider a situation where we have vacuous prior knowledge of X with Ω X = { x 1 , … , x 70 } and we receive evidence represented as BPA m for X as follows: m ( { x 1 } ) = 0.30 , m ( { x 2 } ) = 0.01 , and m ( { x 2 , … , x 70 } ) = 0.69 . Then B e t P m is as follows: B e t P m ( x 1 ) = 0.30 , B e t P m ( x 2 ) = 0.02 , and B e t P m ( x 3 ) = … = B e t P m ( x 70 ) = 0.01 . If B e t P m were appropriate for m , then after receiving evidence m , x 1 is 15 times more likely than x 2 . Now suppose we receive another distinct piece of evidence that is also represented by m . As per the DS theory, our total evidence is now m ⊕ m . If on the basis of m (or B e t P m ), x 1 was 15 times more likely than x 2 , then now that we have evidence m ⊕ m , x 1 should be even more likely (exactly 15 2 = 225 times) than x 2 . But B e t P m ⊕ m ( x 1 ) ≈ 0.156 and B e t P m ⊕ m ( x 2 ) ≈ 0.036 . So according to B e t P m ⊕ m , x 1 is only 4.33 more likely than x 2 . This implies that the second piece of evidence favors x 2 over x 1 (by a factor of 15 / 4.33 = 3.46 ). But the two distinct pieces of evidence are represented by the same BPA. This doesn't make much sense, and the only rational conclusion is that B e t P m is inconsistent with Dempster's combination rule.

Next, let's define maximum entropy credal set transform, C r P m . Suppose m is a BPA for X . Then C r P m is a PMF for X defined as follows: (20) C r P m = arg ⁡ max P X ∈ P m ⁡ H s ( P X ) . In words, C r P m is the PMF of X that has the highest Shannon entropy of all PMFs in the credal set P m . Regarding numerical computation of the first component of C r P m , which involves nonlinear optimization , some algorithms are described in [32] , [34] , [18] , [29] , [5] .

The following example illustrates the C r P m transform, and shows that it does not satisfy the consistency with DS theory semantics requirement P m 1 ⊕ m 2 = P m 1 ⊗ P m 2 .

Example 3

This example is adapted from [51] . A mafia boss has decided to assassinate Mr. Jones. He has three assassins on his payroll, Peter ( pe ), Paul ( pa ), and Mary ( ma ). We have two pieces of distinct evidence. First, the mafia boss will toss a fair coin to decide on the assassin—if the toss results in heads, he will pick either pe or pa , and we know nothing about the process of picking pe or pa . If the toss results in tails, he will pick ma . This piece of evidence can be represented by a BPA m 1 for K ( Ω K = { p e , p a , m a } ) such that m 1 ( { p e , p a } ) = 0.5 , m 1 ( { m a } ) = 0.5 . The second piece of evidence is that Peter has a perfect alibi, and therefore cannot be the killer of Mr. Jones. This piece can be modeled by the BPA m 2 for K such that m 2 ( { p a , m a } ) = 1 . Mr Jones is found dead. The main question of interest is: Who killed Mr. Jones?

For m 1 , P m 1 = { P ∈ P : P ( p e ) + P ( p a ) = 0.5 , P ( m a ) = 0.5 } , and C r P m 1 is as follows: C r P m 1 ( p e ) = 0.25 , C r P m 1 ( p a ) = 0.25 , and C r P m 1 ( m a ) = 0.50 . For m 2 , P m 2 = { P ∈ P : P ( p e ) = 0 } , and C r P m 2 is as follows: C r P m 2 ( p e ) = 0 , C r P m 2 ( p a ) = 0.50 , and C r P m 2 ( m a ) = 0.50 .

m 1 ⊕ m 2 is as follows: ( m 1 ⊕ m 2 ) ( { p a } ) = 0.5 , and ( m 1 ⊕ m 2 ) ( { m a } ) = 0.5 . P m 1 ⊕ m 2 = { P ∈ P : P ( p a ) = 0.5 , P ( m a ) = 0.5 } , which is a singleton subset. Therefore, C r P m 1 ⊕ m 2 is such that C r P m 1 ⊕ m 2 ( p e ) = 0 , C r P m 1 ⊕ m 2 ( p a ) = 0.5 , and C r P m 1 ⊕ m 2 ( m a ) = 0.5 .

Notice that C r P m 1 ⊗ C r P m 2 is as follows: ( C r P m 1 ⊗ C r P m 2 ) ( p e ) = 0 , ( C r P m 1 ⊗ C r P m 2 ) ( p a ) = 1 / 3 , and ( C r P m 1 ⊗ C r P m 2 ) ( m a ) = 2 / 3 , which is different from C r P m 1 ⊕ m 2 . Thus, C r P m is inconsistent with Dempster's combination rule.

Finally, let's define the plausibility transform [55] , [6] . Suppose m is a BPA for X . The plausibility transform, denoted by P l _ P m , is based on the plausibility function P l m corresponding to m , and is defined as follows: (21) P l _ P m ( x ) = K − 1 ⋅ P l m ( { x } ) = K − 1 ⋅ Q m ( { x } ) for all x ∈ Ω X , where K is a normalization constant that ensures P l _ P m is a PMF, i.e., K = ∑ x ∈ Ω X P l m ( { x } ) = ∑ x ∈ Ω X Q m ( { x } ) .

[6] argues that of the many methods for transforming belief functions to PMFs, the plausibility transform is one that is consistent with Dempster's combination rule in the sense that if we have BPAs m 1 , … , m k for X , then P l _ P m 1 ⊕ … ⊕ m k = P l _ P m 1 ⊗ … ⊗ P l _ P m k , where ⊗ denotes Bayes combination rule (pointwise multiplication followed by normalization). It can be shown that the plausibility transform is the only transform that has this property, which follows from the fact that for singleton subsets, the values of the plausibility function P l m are equal to the values of the commonality function Q m , and the fact that Dempster's combination rule is pointwise multiplication of commonality functions followed by normalization (Eq. (14) ).

Example 4

Consider a BPA m for X as described in Example 2 as follows: m ( { x 1 } ) = 0.30 , m ( { x 2 } ) = 0.01 , m ( { x 2 , … , x 70 } ) = 0.69 . Then, P l m for singleton subsets is as follows: P l m ( { x 1 } ) = 0.30 , P l m ( { x 2 } ) = 0.70 , P l m ( { x 3 } ) = ⋯ = P l m ( { x 70 } ) = 0.69 . The plausibility transform of m is as follows: P l _ P m ( x 1 ) = 0.3 / 49.72 ≈ 0.0063 , and P l _ P m ( x 2 ) = 0.7 / 49.72 ≈ 0.0146 , and P l _ P m ( x 3 ) = ⋯ = P l _ P m ( x 70 ) ≈ 0.0144 . Notice that P l _ P m is qualitatively different from B e t P m . In B e t P m , x 1 is 15 times more likely than x 2 . In P l _ P m , x 2 is 2.33 times more likely than x 1 .

Now suppose we get a distinct piece of evidence that is identical to m , so that our total evidence is m ⊕ m . If we compute m ⊕ m and P l _ P m ⊕ m , then as per P l _ P m ⊕ m , x 2 is 2.33 2 more likely than x 1 . This is a direct consequence of the consistency of the plausibility transform with Dempster's combination rule.

One practical use of an uncertainty theory is to make decisions under uncertainty. To achieve this, we must first agree on the semantics of the theory. The semantics of the DS belief function theory cannot be “a matter of personal opinion” [50] . For the BPA m in Example 2 , does it mean that x 1 is 15 times more probable than x 2 (as suggested by the pignistic transform)? Or does it mean that x 2 is 2.33 more probable than x 1 (as suggested by the plausibility transform)? One way to decide is to base our decisions on the center-piece of the DS theory, Dempster's combination rule. It is Dempster's combination rule that distinguishes the DS theory from the Fagin–Halpern theory, which views a belief function as a credal set of PMFs.

There are, of course, semantics that are consistent with DS theory, such as multivalued mappings [9] , random codes [43] , transferable beliefs [51] , and hints [27] .

Desired properties of entropy of BPAs in the DS theory. The following is a list of six desired properties of entropy H ( m ) , where m is a BPA. Most of these are motivated by the properties of Shannon's entropy of PMFs described in Section 2 . Before listing the properties, let us emphasize that we implicitly assume existence and continuity—given a BPA m , H ( m ) should always exist, and H ( m ) should be a continuous function of m . We do not list these two requirements explicitly.

Let X and Y denote random variables with state spaces Ω X and Ω Y , respectively. Let m X and m Y denote distinct BPAs for X and Y , respectively. Let ι X and ι Y denote the vacuous BPAs for X and Y , respectively.

    1.

    ( Consistency with DS theory semantics ) If a definition of entropy of m , or a portion of a definition, is based on a transform of BPA m to a PMF P m , then the transform must satisfy the condition P m 1 ⊕ m 2 = P m 1 ⊗ P m 2 . Notice that this property is not postulating the use of a probability transform. Only that if a transform is used, then it must be consistent with Dempster's rule. As the plausibility transform is the only one that satisfies this property, any definition that uses a transform different from the plausibility transform will not satisfy this property.
    2.

    ( Non-negativity ) H ( m X ) ≥ 0 , with equality if and only if there is a x ∈ Ω X such that m X ( { x } ) = 1 . This is similar to the probabilistic case.
    3.

    ( Maximum entropy ) H ( m X ) ≤ H ( ι X ) , with equality if and only if m X = ι X . This makes sense as the vacuous BPA ι X for X has the most uncertainty among all BPAs for X . Such a property is also advocated in [4] .
    4.

    ( Monotonicity ) If | Ω X | < | Ω Y | , then H ( ι X ) < H ( ι Y ) . This is similar to Axiom 2 of Shannon.
    5.

    ( Probability consistency ) If m X is a Bayesian BPA for X , then H ( m X ) = H s ( P X ) , where P X is the PMF of X corresponding to m X , i.e., P X ( x ) = m X ( { x } ) for all x ∈ Ω X , and H s ( P X ) is Shannon's entropy of PMF P X . In other words, if m X is a Bayesian BPA for X , then H ( m X ) = ∑ x ∈ Ω X m X ( { x } ) log ⁡ ( 1 m X ( { x } ) ) .
    6.

    ( Additivity ) Having distinct BPAs m X and m Y for X and Y , respectively, we can combine them using Dempster's rule yielding BPA m X ⊕ m Y for { X , Y } . Then, (22) H ( m X ⊕ m Y ) = H ( m X ) + H ( m Y ) . This is a weak version of the compound axiom for Shannon's entropy of a PMF (for the case of independent random variables).

The additivity property is quite weak, and is satisfied by most definitions of entropy that are on a log scale. The consistency with DS theory semantics property helps to bolster the additivity property, and ensures that any definition of entropy for m in the DS theory is consistent with Dempster's combination rule. As we will see in Section 5 , not all previous definitions in the literature are consistent with Dempster's combination rule, even though they satisfy the additivity property.

Klir and Wierman [26] also describe a set of properties that they believe should be satisfied by any meaningful measure of uncertainty based on intuitive grounds. Some of the properties that they suggest are also included in the above list. For example, probability consistency and additivity appear in both sets of requirements. Nevertheless, two of them do not make intuitive sense to us.

First, Klir and Wierman suggest a property that they call “set consistency” as follows:

    7.

    ( Set consistency ) H ( m ) = log ⁡ ( | a | ) whenever m is deterministic with focal set a , i.e., m ( a ) = 1 .

This property would require that H ( ι X ) = log ⁡ ( | Ω X | ) . The probability consistency property would require that for the Bayesian uniform BPA m u , H ( m u ) = log ⁡ ( | Ω X | ) . Thus, these two requirements would entail that H ( ι X ) = H ( m u ) = log ⁡ ( | Ω X | ) . We disagree. Recall the Ellsberg paradox [14] phenomenon described in Section 1 , also called ambiguity aversion . According to our requirements, H ( ι X ) > H ( m u ) , which make more intuitive sense than requiring H ( ι X ) = H ( m u ) . The Ellsberg paradox phenomenon is an argument in favor of our requirements. The persons who prefer the urn with 50 red balls and 50 blue balls (whose uncertainty is described by H ( m u ) ) to the urn with 100 total balls for which the number of blue or red balls is unknown (whose uncertainty is described by H ( ι X ) ) do so because they are convinced that there is less uncertainty in H ( m u ) than in H ( ι X ) .

Second, Klir and Wierman require a property they call “range” as follows:

    8.

    ( Range ) For any BPA m X for X , 0 ≤ H ( m X ) ≤ log ⁡ ( | Ω X | ) .

The probability consistency property requires that H ( m u ) = log ⁡ ( | Ω X | ) . Also including the range property prevents us, e.g., from having H ( ι X ) > H ( m u ) . So we do not include it in our list as it violates our intuition.

Finally, Klir and Wierman require the subadditivity property defined as follows.

    9.

    ( Subadditivity ) Suppose m is a BPA for { X , Y } , with marginal BPAs m ↓ X for X , and m ↓ Y for Y . Then, (23) H ( m ) ≤ H ( m ↓ X ) + H ( m ↓ Y ) .

This property is the analog of the corresponding property for Shannon's entropy for probability distribution. We agree that it is an important property, and the only reason we do not include it in our list is because we are unable to meet this requirement in addition to the six requirements that we do include.

Abellán and Moral [4] interpret a BPA m as a credal set of PMFs as in Eq. (18) . With this interpretation, they propose a set monotonicity property as follows.

    10.

    ( Set monotonicity ) If m 1 and m 2 are BPA functions for X with credal sets P m 1 and P m 2 , respectively, such that P m 1 ⊆ P m 2 , then H ( m 1 ) ≤ H ( m 2 ) .

If the credal set semantics of a BPA function were appropriate for the DS theory, then it would be reasonable to adopt the set monotonicity property. However, as we have argued earlier, credal set semantics are not compatible with Dempster's combination rule. If our current knowledge of X is represented by BPA m 1 , and we obtain a piece of evidence represented by BPA m 2 for X that is distinct from m 1 , then in the DS theory, our new knowledge is represented by m 1 ⊕ m 2 . In general, it is not possible to formulate any relationship between P m 1 and P m 1 ⊕ m 2 . For these reasons, we do not adopt Abellán–Moral's set monotonicity property.

5. Previous definitions of entropy of BPAs in the DS theory

In this section, we review all previous definitions of entropy of BPAs in the DS theory of which we are aware. We also verify whether or not these previous definitions satisfy the six basic properties described in Section 4 .

Höhle. One of the earliest definitions of entropy for DS theory is due to Höhle [20] , who defines entropy of BPA m as follows. Suppose m is a BPA for X with state space Ω X . (24) H o ( m ) = ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( 1 B e l m ( a ) ) , where B e l m denotes the belief function corresponding to m as defined in Eq. (9) . H o ( m ) captures only the conflict measure of uncertainty. H o ( ι X ) = 0 . Thus, H o does not satisfy non-negativity, maximum entropy, and monotonicity properties. For Bayesian BPA, m ( { x } ) = B e l m ( { x } ) , and therefore, H o does satisfy the consistency with DS theory semantics and probability consistency property. It satisfies the additivity property but not the subadditivity property [13] .

Smets. Smets [48] defines entropy of BPA m as follows. Suppose m is a non-dogmatic BPA for X , i.e., m ( Ω X ) > 0 . Let Q m denote the commonality function corresponding to BPA m . As m is non-dogmatic, it follows that Q m ( a ) > 0 for all a ∈ 2 Ω X . The entropy of m is as follows: (25) H t ( m ) = ∑ a ∈ 2 Ω X log ⁡ ( 1 Q m ( a ) ) . If m is dogmatic, H t ( m ) is defined as +∞. Smets' definition H t ( m ) is designed to measure “information content” of m , rather than uncertainty. Like Höhle's definition, H t ( ι X ) = 0 , and therefore, H t does not satisfy the non-negativity, maximum entropy, and monotonicity properties. As a Bayesian BPA is not non-dogmatic, the probabilistic consistency property is not satisfied either. If m 1 and m 2 are two non-conflicting (i.e., normalization constant in Dempster's combination rule K = 1 ) and non-dogmatic BPAs, then H t ( m 1 ⊕ m 2 ) = H t ( m 1 ) + H t ( m 2 ) . Thus, it satisfies the additivity property for the restricted class of non-dogmatic BPAs. It also satisfies the consistency with DS theory semantics property. It does not satisfy the subadditivity property [13] .

Yager. Another definition of entropy of BPA m is due to Yager [57] : (26) H y ( m ) = ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( 1 P l m ( a ) ) , where P l m is the plausibility function corresponding to m as defined in Eq. (8) . Yager's definition H y ( m ) measures only conflict in m , not total uncertainty. Like Höhle's and Smets' definitions, H y ( ι X ) = 0 , and therefore, H y does not satisfy the non-negativity, maximum entropy, and monotonicity properties. It does satisfy the probability consistency property because for Bayesian BPA, P l m ( { x } ) = m ( { x } ) . It satisfies the consistency with DS theory semantics, the additivity property, but not the subadditivity property [13] .

Nguyen. Nguyen [35] defines entropy of BPA m for X as follows: (27) H n ( m ) = ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( 1 m ( a ) ) The same definition is stated in [33] . Like all previous definitions, it captures only the conflict portion of uncertainty. As in the previous definitions, H n ( ι X ) = 0 . Thus, H n does not satisfy the non-negativity, maximum entropy, and monotonicity properties. However, as it immediately follows from the properties of Shannon's entropy, it does satisfy the probabilistic consistency property. The fact that it also satisfies the additivity property follows from the fact that log of a product is the sum of the logs. Thus, H ( m X ⊕ m Y ) = ∑ a ∈ 2 Ω { X , Y } m X ( a ↓ X ) m Y ( a ↓ Y ) log ⁡ ( 1 m X ( a ↓ X ) m Y ( a ↓ Y ) ) = ( ∑ a ↓ X ∈ 2 Ω X m X ( a ↓ X ) log ⁡ ( 1 m X ( a ↓ X ) ) ) + ( ∑ a ↓ Y ∈ 2 Ω Y m Y ( a ↓ Y ) log ⁡ ( 1 m Y ( a ↓ Y ) ) ) = H ( m ↓ X ) + H ( m ↓ Y ) . It satisfies the consistency with DS theory semantics property, but not the subadditivity property as can be seen from Example 5 .

Example 5

Consider BPA m for { X , Y } as follows: m ( { ( x , y ) , ( x ¯ , y ¯ ) } ) = m ( { ( x , y ¯ ) , ( x ¯ , y ) } ) = 1 2 . For this BPA, H n ( m ) = 1 . Also, m ↓ X = ι X , and m ↓ Y = ι Y . Therefore, H n ( m ↓ X ) = 0 , and H n ( m ↓ Y ) = 0 . Thus, subadditivity is not satisfied.

Dubois and Prade. Dubois and Prade [13] define entropy of BPA m for X as follows: (28) H d ( m ) = ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( | a | ) . Dubois–Prade's definition captures only the non-specificity portion of uncertainty. If X is a random variable with state space Ω X , Hartley [19] defines a measure of entropy of X as log ⁡ ( | Ω X | ) . Dubois–Prade's definition H d ( m ) can be regarded as the mean of Hartley entropy of m . If ι X denotes the vacuous BPA for X , then H d ( ι X ) = log ⁡ ( | Ω X | ) . If m is a Bayesian BPA, then H d ( m ) = 0 as all the focal elements of m are singletons. Thus, H d satisfies the consistency with DS theory semantics, maximum entropy, and monotonicity properties, but it does not satisfy the non-negativity and probabilistic consistency properties. However, it does satisfy the additivity and subadditivity properties [13] . Ramer [39] proves that H d is the unique definition of non-specificity entropy of m that satisfies additivity and the subadditivity properties.

Lamata and Moral. Lamata and Moral [28] suggest a definition of entropy of BPA m as follows: (29) H l ( m ) = H y ( m ) + H d ( m ) , which combines Yager's definition H y ( m ) as a measure of conflict, and Dubois–Prade's definition H d ( m ) as a measure of non-specificity. It is easy to verify that H l ( ι X ) = H l ( m u ) = log ⁡ ( | Ω X | ) , which violates the maximum entropy property. It satisfies the consistency with DS theory semantics, non-negativity, monotonicity, probability consistency, and additivity, properties. It does not satisfy the subadditivity property [13] .

Klir and Ramer. Klir and Ramer [25] define entropy of BPA m for X as follows: (30) H k ( m ) = ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( 1 1 − ∑ b ∈ 2 Ω X m ( b ) | b ∖ a | | b | ) + H d ( m ) . The first component in Eq. (30) is designed to measure conflict, and the second component is designed to measure non-specificity. It is easy to verify that H k ( ι X ) = H k ( m u ) = log ⁡ ( | Ω X | ) , which violates the maximum entropy property. It satisfies the consistency with DS theory semantics, non-negativity, monotonicity, probability consistency, and additivity, properties. It does not satisfy the subadditivity property [53] .

Klir and Parviz. Klir and Parviz [24] modify Klir and Ramer's definition H k ( m ) slightly to measure conflict in a more refined way. The revised definition is as follows: (31) H p ( m ) = ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( 1 1 − ∑ b ∈ 2 Ω X m ( b ) | a ∖ b | | a | ) + H d ( m ) . Klir and Parviz argue that the first component in Eq. (31) is a better measure of conflict than the first component in Eq. (30) . Like H k ( m ) , H p ( m ) satisfies the consistency with DS theory semantics, non-negativity, monotonicity, probability consistency, and additivity properties, but not the maximum entropy, and subadditivity [54] properties.

Pal et al. Pal et al. [36] , [37] define entropy H b ( m ) as follows: (32) H b ( m ) = ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( | a | m ( a ) ) . H b ( m ) satisfies consistency with DS theory semantics, non-negativity, monotonicity, probability consistency, and additivity [37] , properties. H b ( ι X ) = H b ( m u ) = log ⁡ ( | Ω X | ) . Thus, it does not satisfy the maximum entropy property. The maximum value of H b ( m ) is attained for m such that m ( a ) ∝ | a | , for all a ∈ 2 Ω X . Thus, for a binary-valued variable X , the maximum value of H b ( m ) is 2 whereas H b ( ι X ) = 1 .

Maeda and Ichihashi. Maeda–Ichihashi [31] define H i ( m ) using the credal set P m semantics of m described in Section 4 as follows: (33) H i ( m ) = max P X ∈ P m ⁡ { H s ( P X ) } + H d ( m ) = H s ( C r P m ) + H d ( m ) where the first component is interpreted as a measure of conflict only, and the second component is interpreted as a measure of non-specificity. H i ( m ) satisfies all properties including the subadditivity property described in Eq. (23) [31] . As discussed in Section 4 , the maximum entropy credal set transform C r P m is not consistent with Dempster's combination rule. H i ( m ) may be appropriate for a theory of belief functions interpreted as a credal set with the Fagin–Halpern combination rule. It is, however, inappropriate for the Dempster–Shafer theory of belief functions with Dempster's rule as the rule for combining (or updating) beliefs.

Harmanec and Klir. Harmanec–Klir [17] define H h ( m ) as follows: (34) H h ( m ) = max P X ∈ P m ⁡ H s ( P X ) = H s ( C r P m ) , where they interpret H h ( m ) as a measure of total uncertainty. Abellán [1] interprets min P X ∈ P m ⁡ H s ( P X ) as a measure of conflict, and the difference between H h ( m ) and min P X ∈ P m ⁡ H s ( P X ) as a measure of non-specificity. H h ( ι X ) = H h ( m u ) = log ⁡ ( | Ω X | ) . Thus, it doesn't satisfy the maximum entropy property. It does, however, satisfy all other properties including subadditivity. Like Maeda–Ichihashi's definition, Harmanec–Klir's definition based on the C r P m transform is inconsistent with Dempster's combination rule, and, thus, violates consistency with DS theory semantics property.

Abellán and Moral. Maeda–Ichihashi's definition H i ( m ) does not satisfy the set monotonicity property in Eq. (10) suggested by Abellán–Moral [4] . They suggest a modification of Maeda–Ichihashi's definition in Eq. (33) where they add a third component so that the modified definition satisfies the set monotonicity property in addition to the six properties satisfied by Maeda–Ichihashi's definition. Their definition is as follows: (35) H a ( m ) = H s ( C r P m ) + H d ( m ) + min P X ∈ P m ⁡ K L ( P X , Q X ) , where K L ( P X , Q X ) is the Kullback–Leibler divergence between PMFs P X and Q X defined as follows: (36) K L ( P X , Q X ) = ∑ x ∈ Ω X P X ( x ) ln ⁡ ( P X ( x ) Q X ( x ) ) , and Q X ∈ P m is a PMF of X that has the maximum Shannon entropy in the first term, i.e., H s ( Q X ) = max P X ∈ P m ⁡ { H s ( P X ) } . Like Maeda–Ichihashi's definition, Abellán–Moral's definition does not satisfy the consistency with DS theory semantics property.

Jousselme et al. Jousselme et al. [22] define H j ( m ) based on first transforming a BPA m to a PMF B e t P m using the pignistic transform [12] , [49] , and then using Shannon's entropy of B e t P m . (37) H j ( m ) = H s ( B e t P m ) = ∑ x ∈ Ω X B e t P m ( x ) log ⁡ ( 1 B e t P m ( x ) ) . (A similar definition, called pignistic entropy, appears in [11] in the context of the Dezert–Smarandache theory, which can be considered a generalization of the DS belief functions theory.) H j ( m ) satisfies the non-negativity, monotonicity, probability consistency, and additivity properties [22] . It does not satisfy the maximum entropy property as H j ( ι X ) = H j ( m u ) = log ⁡ ( | Ω X | ) . Although Jousselme et al. claim that H j ( m ) satisfies the subadditivity property (Eq. (23) ), a counter-example is provided in [23] . One basic assumption behind H j ( m ) is that B e t P m is an appropriate probabilistic representation of the uncertainty in m in the DS theory. As we have argued in Section 4 , B e t P m is inconsistent with Dempster's combination rule.

Pouly et al. Pouly et al. [38] define entropy of a “hint” associated with a BPA m . A hint is a formalization of the multivalued mapping semantics for BPAs, and is more fine-grained than a BPA. Formally, a hint H = ( Ω 1 , Ω 2 , P , Γ ) consists of two state spaces Ω 1 and Ω 2 , a PMF P on Ω 1 , and a multivalued mapping Γ : Ω 1 → 2 Ω 2 . The PMF P and multivalued mapping Γ induces a BPA m for Ω 2 such that m ( Γ ( θ 1 ) ) = P ( θ 1 ) . An example of a hint is as follows.

Example 6

A witness claims that he saw the defendant commit a crime. Suppose that we have a PMF on the reliability R of the witness as follows. Let r and r ¯ denote the witness is reliable or not, respectively. Then, P ( r ) = 0.6 , and P ( r ¯ ) = 0.4 . The question of interest, denoted by variable G , is whether the defendant is guilty ( g ) or not ( g ¯ ). If the witness is reliable, then given his or her statement, the defendant is guilty. If the witness is not reliable, then his or her claim has no bearing on the question of guilt of the defendant. Thus, we have a multivalued mapping Γ : { r , r ¯ } → 2 { g , g ¯ } such that Γ ( r ) = { g } , and Γ ( r ¯ ) = { g , g ¯ } . In this example, the hint H = ( { r , r ¯ } , { g , g ¯ } , P , Γ ) . The hint H induces a BPA for G as follows: m ( { g } ) = 0.6 , m ( { g , g ¯ } ) = 0.4 .

Pouly et al.'s definition of entropy of hint H = ( Ω 1 , Ω 2 , P , Γ ) is as follows: (38) H r ( H ) = H s ( P ) + H d ( m ) , where m is the BPA on state space Ω 2 induced by hint H . The expression in Eq. (38) is derived using Shannon's entropy of a joint PMF on the space Ω 1 × Ω 2 whose marginal for Ω 1 is P , and an assumption of uniform conditional PMF for Γ ( ω ) ⊆ Ω 2 given ω ∈ Ω 1 . This assumption results in a marginal PMF for Ω 2 that is equal to B e t P m , where m is the BPA on state space Ω 2 induced by hint H . Dempster's combination rule never enters the picture in the derivation on H r ( H ) . H r ( H ) has nice properties (on the space of hints). H r ( H ) is on the scale [ 0 , log ⁡ ( | Ω 1 | ) + log ⁡ ( | Ω 2 | ) ] . For a BPA m defined on the state space Ω 2 , it would make sense to use only the marginal of the joint PMF on Ω 1 × Ω 2 for Ω 2 , which is B e t P m . Thus, if one were to adapt Pouly et al.'s definition for BPAs, it would coincide with the Jousselme et al.'s definition, i.e., (39) H r ( m ) = H j ( m ) = H s ( B e t P m ) = ∑ θ ∈ Ω 2 B e t P m ( θ ) log ⁡ ( 1 B e t P m ( θ ) ) . Thus, Pouly et al.'s definition of entropy of BPA m has the same properties as Jousselme et al.'s definition.

Deng. Deng [10] proposes a definition of entropy of BPA m for X as follows: (40) H g ( m ) = H n ( m ) + ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( 2 | a | − 1 ) The first component, Nguyen's definition of entropy, is a measure of conflict, and the second component is a measure of non-specificity. Deng's definition satisfies the probability consistency property. Abellán [2] shows that Deng's definition does not satisfy monotonicity, additivity, and subadditivity properties. It also does not satisfy the maximum entropy property.

A summary of the properties of the various definitions of entropy of DS belief functions is shown in Table 1 .

Table 1 . A summary of the six desired properties and subadditivity of the various definitions of entropy of DS belief functions.
Definition	Cons. with DS	Non-neg.	Max. ent.	Monoton.	Prob. cons.	Additivity	Subadd.
Höhle, Eq. (24) 	yes	no	no	no	yes	yes	no
Smets, Eq. (25) 	yes	no	no	no	no	yes	no
Yager, Eq. (26) 	yes	no	no	no	yes	yes	no
Nguyen, Eq. (27) 	yes	no	no	no	yes	yes	no
Dubois–Prade, Eq. (28) 	yes	no	yes	yes	no	yes	yes
Lamata–Moral, Eq. (29) 	yes	yes	no	yes	yes	yes	no
Klir–Ramer, Eq. (30) 	yes	yes	no	yes	yes	yes	no
Klir–Parviz, Eq. (31) 	yes	yes	no	yes	yes	yes	no
Pal et al., Eq. (32) 	yes	yes	no	yes	yes	yes	no
Maeda–Ichihashi, Eq. (33) 	no	yes	yes	yes	yes	yes	yes
Harmanec–Klir, Eq. (34) 	no	yes	no	yes	yes	yes	yes
Abellán–Moral, Eq. (35) 	no	yes	yes	yes	yes	yes	yes
Jousselme et al., Eq. (37) 	no	yes	no	yes	yes	yes	no
Pouly et al., Eq. (39) 	no	yes	no	yes	yes	yes	no
Deng, Eq. (40) 	yes	yes	no	no	yes	no	no
6. A new definition of entropy for DS theory

In this section, we propose a new definition of entropy for DS theory. The new definition of entropy is based partially on the plausibility transform.

A new definition of entropy of a BPA. To explain the basic idea behind the following definition consider a simple example with an urn containing n balls of up to two colors: white ( w ), and black ( b ). Suppose we draw a ball at random from the urn and X denotes its color. What is the entropy of the BPA for X in the situation where we know that there is at least one ball of each color in the urn? The simplest case is when n = 2 . In this case the entropy is exactly the same as in tossing a fair coin: log ⁡ ( 2 ) = 1 . Naturally, the greater n is, the greater uncertainty in the model. As there is no information preferring one color to another one, the only probabilistic description of the model is a uniform PMF. In DS theory, the BPA describing this situation is m ( { w } ) = m ( { b } ) = 1 n , and m ( { w , b } ) = n − 2 n . Therefore, the entropy function for this BPA must be greater than or equal to Shannon's entropy of a uniform PMF with two states ( log ⁡ ( 2 ) = 1 ), and increasing with increasing n . This is why the following definition of entropy of a BPA m consists of two components. The first component is Shannon's entropy of a PMF that corresponds to m , and the second component includes entropy associated with non-singleton focal sets of m .

Suppose m is a BPA for X . The entropy of m is defined as follows: (41) H ( m ) = H s ( P l _ P m ) + H d ( m ) = ∑ x ∈ Ω X P l _ P m ( x ) log ⁡ ( 1 P l _ P m ( x ) ) + ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( | a | ) .

Like some of the definitions in the literature, the first component in Eq. (41) is designed to measure conflict in m , and the second component is designed to measure non-specificity in m . Both components are on the scale [ 0 , log ⁡ ( | Ω X | ) ] , and therefore, H ( m ) is on the scale [ 0 , 2 log ⁡ ( | Ω X | ) ] .

Theorem 1

The entropy H ( m ) for BPA m for X defined in Eq. (41) satisfies the consistency with DS theory semantics, non-negativity, maximum entropy, monotonicity, probability consistency, and additivity properties.

Proof

The entropy H ( m ) has two components, both of which are consistent with DS theory semantics. Thus, it satisfies the consistency with DS theory semantics property.

We know that H s ( P l _ P m ) ≥ 0 , and H d ( m ) ≥ 0 . Thus, H ( m ) ≥ 0 . For H ( m ) = 0 to hold, both H s ( P l _ P m ) = 0 , and H d ( m ) = 0 must be satisfied. H s ( P l _ P m ) = 0 if and only if there exists x ∈ Ω X such that P l _ P m ( x ) = 1 , which occurs if and only if m ( { x } ) = 1 . H d ( m ) = 0 if and only if m is Bayesian. Thus, H ( m ) satisfies the non-negativity property.

Let n denote | Ω X | . Then P P l ι X ( x ) = 1 n for all x ∈ Ω X , and therefore H s ( P P l ι X ) = log ⁡ ( n ) , which is the maximum of all PMFs defined on Ω X . Also H d ( ι X ) = log ⁡ ( n ) , which is the maximum of Dubois–Prade's entropy over all BPAs m for X . Thus, H ( m ) satisfies the maximum entropy property.

H ( ι X ) = 2 log ⁡ ( | Ω X | ) . Thus, since it is monotonic in | Ω X | , H ( m ) satisfies the monotonicity property.

If m is Bayesian, then P l _ P m ( x ) = m ( { x } ) for all x ∈ Ω X , and H d ( m ) = 0 . Thus, H ( m ) satisfies the probability consistency property.

Suppose m X is a BPA for X , and m Y is a BPA for Y . Then, as it is shown in [6] , P P l m X ⊕ m Y = P P l m X ⊗ P P l m Y , and the normalization constant in the case of PMFs for disjoint arguments is 1. Thus, H s ( P P l m X ⊕ m Y ) = H s ( P P l m X ) + H s ( P P l m Y ) . Also, it is proved in [13] , that H d ( m X ⊕ m Y ) = H d ( m X ) + H d ( m Y ) . Thus, H ( m ) satisfies the additivity property.  □

The additivity property was stated in terms of BPAs m X for X and m Y for Y . Suppose we have a set of variables, say v , and r , s ⊆ v . This property could have been stated more generally in terms of BPAs m 1 for r and m 2 for s where r ∩ s = ∅ . In this case still H ( m 1 ⊕ m 2 ) = H ( m 1 ) + H ( m 2 ) because both components of the new definition (i.e., H s and H d ) satisfy the more general property. However, if r ∩ s ≠ ∅ , then generally H ( m 1 ⊕ m 2 ) may be different from H ( m 1 ) + H ( m 2 ) . This is because neither the first component of the new definition, nor the Dubois–Prade component, satisfy the stronger property. An example illustrating this is described next.

Example 7

Consider BPA m 1 for binary-valued variable X as follows: m 1 ( { x } ) = 0.1 , m 1 ( { x ¯ } ) = 0.2 , m 1 ( Ω X ) = 0.7 , and BPA m 2 for { X , Y } as follows: m 2 ( { ( x , y ) , ( x ¯ , y ) } ) = 0.08 , m 2 ( { ( x , y ) , ( x ¯ , y ¯ ) } ) = 0.72 , m 2 ( { ( x , y ¯ ) , ( x ¯ , y ) } ) = 0.02 , m 2 ( { ( x , y ¯ ) , ( x ¯ , y ¯ ) } ) = 0.18 .

Assuming these two BPAs represent distinct pieces of evidence, we can combine them with Dempster's rule obtaining m = m 1 ⊕ m 2 for { X , Y } as follows: m ( { ( x , y ) } ) = 0.08 , m ( { ( x , y ¯ ) } ) = 0.02 , m ( { ( x ¯ , y ) } ) = 0.02 , m ( { ( x ¯ , y ¯ ) } ) = 0.18 , m ( { ( x , y ) , ( x ¯ , y ) } ) = 0.056 , m ( { ( x , y ) , ( x ¯ , y ¯ ) } ) = 0.504 , m ( { ( x , y ¯ ) , ( x ¯ , y ) } ) = 0.014 , m ( { ( x , y ¯ ) , ( x ¯ , y ¯ ) } ) = 0.126 .

Now, the PMF P l _ P m 1 of X obtained using the plausibility transform of m 1 is as follows:

P l _ P m 1 ( x ) = 0.47 , and P l _ P m 1 ( x ¯ ) = 0.53 , and its Shannon's entropy is H s ( P l _ P m 1 ) = 0.998 . H d ( m 1 ) = 0.7 . Thus, H ( m 1 ) = 1.698 .

The PMF P l _ P m 2 of { X , Y } obtained using the plausibility transform is as follows:

P l _ P m 2 ( x , y ) = 0.4 , P l _ P m 2 ( x , y ¯ ) = 0.1 , P l _ P m 2 ( x ¯ , y ) = 0.05 , P l _ P m 2 ( x ¯ , y ¯ ) = 0.45 , and its Shannon's entropy is H s ( P l _ P m 2 ) = 1.595 . H d ( m 2 ) = 1 . Thus, H ( m 2 ) = 2.595 .

The joint PMF of { X , Y } obtained using the plausibility transform is as follows:

P l _ P m ( x , y ) = 0.38 , P l _ P m ( x , y ¯ ) = 0.09 , P l _ P m ( x ¯ , y ) = 0.05 , P l _ P m ( x ¯ , y ¯ ) = 0.48 , and its Shannon's entropy is H ( P l _ P m ) = 1.586 . Also, Dubois–Prade's entropy of m is H d ( m ) = 0.7 . Thus, H ( m ) = 2.286 .

Notice that H ( m ) = 2.286 ≠ H ( m 1 ) + H ( m 2 ) = 1.698 + 2.595 = 4.293 , H ( P l m ) = 1.586 ≠ H ( P P l m 1 ) + H ( P P l m 2 ) = 0.998 + 1.595 = 2.593 , and H d ( m ) = 0.7 ≠ H d ( m 1 ) + H d ( m 2 ) = 0.7 + 1 = 1.7 .

7. Additional properties of H ( m )

In this section, we describe some additional properties of H ( m ) defined in Eq. (41) .

Entropy as an expected value. One interpretation of Shannon's entropy in probability theory is that it equals the expected value of information received when learning state x ∈ Ω X , i.e., (42) H s ( P X ) = ∑ x ∈ Ω X P X ( x ) I ( x ) , where I ( x ) = log 2 ⁡ ( 1 P X ( x ) ) represents the information received when learning that state x ∈ Ω X has occurred. Notice that the amount of this information is not the property of state x , but that of its probability.

In the case where our knowledge is encoded by a BPA m (instead of a PMF), we can decompose the information in m into two parts. The first part is the PMF P l _ P m , and the second part (not captured by the first part) is log ⁡ ( | a | ) , which happens with probability m ( a ) . Consider the vacuous BPA function ι X for X , where Ω X = { x , x ¯ } . We can decompose the uncertainty in ι X into the uncertainty in the PMF P P l ι X (which is given by P P l ι X ( x ) = 1 / 2 , and P P l ι X ( x ¯ ) = 1 / 2 ). But this doesn't capture the entire uncertainty in ι X . We also have to include the uncertainty log ⁡ ( | Ω X | ) . The expected value of the first part is Shannon's entropy H ( P P l ι X ) = 1 bit, and the expected value of the second is ι X ( Ω X ) log ⁡ ( | Ω X | ) = 1 bit.

Thus, we can interpret H ( m ) as an expected value, but with respect to two different sources of uncertainty. The first part is expected value of information I ( x ) with respect to PMF P l _ P m , and the second part is expected value of information necessary to eliminate the uncertainty emerging from the size of Ω X , i.e., log ⁡ ( | a | ) , with respect to “distribution” m , i.e., ∑ a ∈ 2 Ω X m ( a ) log ⁡ ( | a | ) . The second part corresponds to the measure of uncertainty suggested by Richard Hartley in 1928 [19] , about which Rényi showed that it is the only one satisfying additivity and monotonicity properties (for a precise formulation of this property see [40] ). Notice that both parts are measured in same units (bits), and it makes sense to add the two.

Subadditivity property. As shown in Example 8 below, our definition does not satisfy the subadditivity property in Eq. (23) .

Example 8

Consider a two-dimensional BPA m for binary-valued variables { X , Y } with five focal elements: m ( { ( x , y ) } ) = m ( { ( x , y ¯ ) } ) = 0.1 , m ( { ( x ¯ , y ) } ) = m ( { ( x ¯ , y ¯ ) } ) = 0.3 , and m ( Ω { X , Y } ) = 0.2 . The joint PMF of { X , Y } using the plausibility transform is as follows: P l _ P m ( ( x , y ) ) = 0.1875 , P l _ P m ( ( x , y ¯ ) ) = 0.1875 , P l _ P m ( ( x ¯ , y ) ) = 0.3125 , P l _ P m ( ( x ¯ , y ¯ ) ) = 0.3125 . Its Shannon's entropy is H s ( P l _ P m ) = 1.9544 . The Dubois–Prade's entropy of m is H d ( m ) = 0.4 . Thus, H ( m ) = 2.3544 .

The marginal BPA m ↓ X is as follows: m ↓ X ( { x } ) = 0.2 , m ↓ X ( { x ¯ } ) = 0.6 , and m ↓ X ( Ω X ) = 0.2 . The PMF P P l m ↓ X of X obtained using the plausibility transform of m ↓ X is as follows: P P l m ↓ X ( x ) = 0.333 , and P P l m ↓ X ( x ¯ ) = 0.667 , and its Shannon's entropy is H s ( P P l m ↓ X ) = 0.9183 .

Similarly, the marginal BPA m ↓ Y is as follows: m ↓ Y ( { y } ) = 0.4 , m ↓ Y ( { y ¯ } ) = 0.4 , and m ↓ Y ( Ω Y ) = 0.2 . The PMF P P l m ↓ Y of Y is as follows: P P l m ↓ Y ( y ) = P P l m ↓ Y ( y ¯ ) = 0.5 , and therefore its Shannon's entropy is H s ( P P l m ↓ Y ) = 1 .

Thus, H s ( P l _ P m ) = 1.9544 > H s ( P P l m ↓ X ) + H s ( P P l m ↓ Y ) = 0.9183 + 1 = 1.9182 . Dubois–Prade's entropies are as follows: H d ( m ↓ X ) = H d ( m ↓ Y ) = 0.2 . Thus, H d ( m ) = 0.4 = H d ( m ↓ X ) + H d ( m ↓ Y ) = 0.2 + 0.2 = 0.4 . Therefore, H ( m ) = 2.3544 > H ( m ↓ X ) + H ( m ↓ Y ) = ( 0.9183 + 0.2 ) + ( 1 + 0.2 ) = 1.1183 + 1.2 = 2.3183 .

Entropy of m ⊕ m . Shannon's entropy of PMFs has the following property: (43) H s ( P X ⊗ P X ) ≤ H s ( P X ) Repetitio est mater studiorum. Learning the same knowledge twice should contribute to our cognizance more than learning it only once. In general, the Bayes combination rule is not idempotent , i.e., P X ⊗ P X ≠ P X . Some PMFs are idempotent. For example, the equally likely PMF, and PMFs that rule out some states and have equally likely probabilities for the others, are idempotent. For non-idempotent PMFs, if we combine P X with itself, then the states with higher probabilities are now more likely, and states with lower probabilities are less likely. Consider the following property of Shannon entropy [52] :

Suppose X is a random variable with state space Ω X = { x 1 , … , x n } , and suppose P 1 and P 2 are PMFs for X such that P 1 ( x i ) = p i and P 2 ( x i ) = q i . Suppose that q 1 ≥ q 2 ≥ … ≥ q n , and p 1 = q 1 − Δ , p 2 = q 2 + Δ , p i = q i for i = 3 , … , n , where 0 ≤ Δ ≤ q 1 . Then H s ( P 2 ) ≥ H s ( P 1 ) .

Using this property repeatedly, it can be shown that the inequality in Eq. (43) holds. One may be tempted to believe that such a property also holds for all BPAs, i.e., H ( m ⊕ m ) ≤ H ( m ) . But, as shown in Example 9 , it is not true.

Example 9

Consider a BPA m for X , where Ω X = { x 1 , x 2 , x 3 } as follows: m ( { x 1 } ) = 1 3 , m ( { x 2 , x 3 } ) = 2 3 . Dubois–Prade's entropy H d ( m ) = 2 3 . Also, for this BPA m , the PMF P l _ P m is as follows: P l _ P m ( x 1 ) = 1 5 , P l _ P m ( x 2 ) = P l _ P m ( x 3 ) = 2 5 . Thus, H s ( P l _ P m ) = 1.522 , and H ( m ) = H s ( P l _ P m ) + H d ( m ) = 2.189 .

If we compute m ⊕ m , we have ( m ⊕ m ) ( { x 1 } ) = 1 5 , and ( m ⊕ m ) ( { x 2 , x 3 } ) = 4 5 . Dubois–Prade's entropy H d ( m ⊕ m ) = 4 5 . Notice that H d ( m ⊕ m ) > H d ( m ) . The PMF P P l m ⊕ m is as follows: P P l m ⊕ m ( x 1 ) = 1 9 , P P l m ⊕ m ( x 2 ) = P P l m ⊕ m ( x 3 ) = 4 9 . And, its Shannon's entropy H s ( P P l m ⊕ m ) = 1.392 . Notice that H s ( P P l m ⊕ m ) < H s ( P l _ P m ) . However, H ( m ⊕ m ) = H s ( m ⊕ m ) + H d ( m ⊕ m ) = 2.192 , which is greater than H ( m ) = 2.189 .

To understand this more intuitively, notice that our definition of entropy H ( m ) has two components. The first one, H s ( P l _ P m ) can be considered a measure of conflict (or confusion or dissonance or discord or strife), and the second one, H d ( m ) can be considered a measure of non-specificity. Thus, while the property in Eq. (43) holds for PMFs, it is not valid for BPAs in the DS theory because of the non-specificity component. When we combine m with itself, probability migrates from subsets with lower plausibility to subsets with larger plausibility [6] . If we have a BPA such that a larger subset has higher plausibility, then H d ( m ⊕ m ) > H d ( m ) .
8. Summary and conclusion

Interpreting Shannon's entropy of a PMF of a discrete random variable as the amount of uncertainty in the PMF [47] , we propose six desirable properties of entropy of a basic probability assignment in the DS theory of belief functions. Four of the six properties are motivated by the analogous properties of Shannon's entropy of PMFs. The maximum entropy property is based on our intuition that a vacuous belief function has more uncertainty than a Bayesian belief function. Some of these six properties are different from the five properties proposed by Klir and Wierman [26] . Two of the properties they require, set consistency and range, are inconsistent with some of the properties we propose. Also, one of the properties that they require, subadditivity, is not included in our set as we are unable to formulate a definition of entropy that would simultaneously satisfy the six properties we suggest plus subadditivity. Also, besides the six properties, we also require that H ( m ) should always exist, and H ( m ) should be a continuous function of m . Thus, a set monotonicity property suggested by Abellán–Masegosa [3] based on credal set semantics of belief functions that are not compatible with Dempster's rule is not included in our set of requirements.

We review some earlier definitions given by Höhle [20] , Smets [48] , Yager [57] , Nguyen [35] , Dubois–Prade [13] , Lamata–Moral [28] , Klir–Ramer [25] , Klir–Parviz [24] , Pal et al. [37] , Maeda–Ichihashi [31] , Abellán–Moral [4] , Harmanec–Klir [17] , Jousselme et al. [22] , Pouly et al. [38] , and Deng [10] . None of these definitions satisfy all the six properties listed earlier. Pouly et al.'s definition is for the joint space of hints, Ω 1 × Ω 2 . If one were to adapt Pouly et al.'s definition for BPAs, then as the marginal entropy for Ω 2 reduces to the pignistic entropy, their definition for BPAs would coincide with that proposed by Jousselme et al.

Smets' definition is motivated by interpreting H ( m ) as a measure of information contained in m , rather than uncertainty. Höhle's, Yager's, and Nguyen's definitions are motivated by interpreting entropy of a BPA as a measure of conflict (or confusion or discord or strife) only. Dubois–Prade's definition is motivated by interpreting entropy of a BPA as a measure of its non-specificity (or imprecision) only.

As first suggested by Lamata and Moral [28] , we propose a new definition of entropy of BPA as a combination of Shannon's entropy of an equivalent PMF that captures the conflict measure of entropy, and Dubois–Prade's entropy of a BPA that captures the non-specificity (or Hartley) measure of entropy. The equivalent PMF is that obtained by using the plausibility transform [55] , [6] . We show that this new definition satisfies all six properties we propose.

One could create a definition, e.g., that combines Jousselme et al.'s definition (Eq. (37) ) with Dubois–Prade's definition (Eq. (28) ), i.e., H ( m ) = H j ( m ) + H d ( m ) , and such a definition would also satisfy five of our six properties, but as we have argued before, the first component, pignistic entropy, is not consistent with semantics for the DS theory.

We also describe some additional properties of our definition of entropy of BPA m . In particular, we describe our definition as the sum of an expected value of Shannon's entropy, which is a measure of conflict, and expected value of Hartley's entropy, which is a measure of non-specificity. We demonstrate that our definition does not satisfy the subadditivity property. This is because the first component, H s ( P l _ P m ) , does not satisfy the subadditivity property. Finally, we show that while Shannon's entropy satisfies the inequality H s ( P X ⊗ P X ) ≤ H ( P X ) , our definition of H ( m ) does not satisfy the corresponding inequality, H ( m ⊕ m ) ≤ H ( m ) . This is because the Dubois–Prade component, generalized Hartley entropy, does not satisfy this inequality, i.e., H d ( m ⊕ m ) may be greater than H d ( m ) .

An open question is whether there exists a definition of entropy of BPA m in the DS theory that satisfies the six properties we list in Section 4 , and the subadditivity property. Our definition satisfies the six properties, but it does not satisfy the subadditivity property.
Acknowledgements

This work has been supported in part by funds from grant GAČR 15-00215S to the first author, and from the Ronald G. Harper Distinguished Professorship at the University of Kansas to the second author. We are grateful to Thierry Denoeux, Marc Pouly, Anne-Laure Jousselme, and Joaquín Abellán for their comments on earlier drafts of this paper. We are also grateful to Suzanna Emelio for a careful proof-reading of the text. We are also grateful to two anonymous reviewers of International Journal of Approximate Reasoning for their constructive comments. A short version of this paper appeared as [21] .

We dedicate this paper to the memory of George J. Klir, who passed away on May 27, 2016.
References

    [1]
    J. Abellán
    Combining nonspecificity measures in Dempster–Shafer theory of evidence
    Int. J. Gen. Syst., 40 (6) (2011), pp. 611-622
    CrossRef View in Scopus Google Scholar
    [2]
    J. Abellán
    Analyzing properties of Deng entropy in the theory of evidence
    Chaos Solitons Fractals, 95 (February 2017), pp. 195-199
    View PDF View article View in Scopus Google Scholar
    [3]
    J. Abellán, A. Masegosa
    Requirements for total uncertainty measures in Dempster–Shafer theory of evidence
    Int. J. Gen. Syst., 37 (6) (December 2008), pp. 733-747
    CrossRef View in Scopus Google Scholar
    [4]
    J. Abellán, S. Moral
    Completing a total uncertainty measure in Dempster–Shafer theory
    Int. J. Gen. Syst., 28 (4–5) (1999), pp. 299-314
    CrossRef View in Scopus Google Scholar
    [5]
    J. Abellán, S. Moral
    An algorithm that computes the upper entropy for order-2 capacities
    Int. J. Uncertain. Fuzziness Knowl.-Based Syst., 14 (2) (2005), pp. 141-154
    Google Scholar
    [6]
    B.R. Cobb, P.P. Shenoy
    On the plausibility transformation method for translating belief function models to probability models
    Int. J. Approx. Reason., 41 (3) (2006), pp. 314-340
    View in Scopus Google Scholar
    [7]
    F. Cuzzolin
    On the relative belief transform
    Int. J. Approx. Reason., 53 (5) (2012), pp. 786-804
    View PDF View article View in Scopus Google Scholar
    [8]
    M. Daniel
    On transformations of belief functions to probabilities
    Int. J. Intell. Syst., 21 (3) (2006), pp. 261-282
    CrossRef View in Scopus Google Scholar
    [9]
    A.P. Dempster
    Upper and lower probabilities induced by a multivalued mapping
    Ann. Math. Stat., 38 (2) (1967), pp. 325-339
    CrossRef Google Scholar
    [10]
    Y. Deng
    Deng entropy
    Chaos Solitons Fractals, 91 (October 2016), pp. 549-553
    View PDF View article View in Scopus Google Scholar
    [11]
    J. Dezert, F. Smarandache, A. Tchamova
    On the Blackman's association problem
    Proceedings of the 6th Annual Conference on Information Fusion, International Society for Information Fusion, Cairns, Queensland, Australia (2003), pp. 1349-1356
    Google Scholar
    [12]
    D. Dubois, H. Prade
    On several representations of an uncertain body of evidence
    M.M. Gupta, E. Sanchez (Eds.), Fuzzy Information and Decision Processes, North-Holland, Amsterdam (1982), pp. 167-181
    View in Scopus Google Scholar
    [13]
    D. Dubois, H. Prade
    Properties of measures of information in evidence and possibility theories
    Fuzzy Sets Syst., 24 (2) (1987), pp. 161-182
    View PDF View article View in Scopus Google Scholar
    [14]
    D. Ellsberg
    Risk, ambiguity and the Savage axioms
    Q. J. Econ., 75 (2) (1961), pp. 643-669
    CrossRef View in Scopus Google Scholar
    [15]
    R. Fagin, J.Y. Halpern
    A new approach to updating beliefs
    P. Bonissone, M. Henrion, L. Kanal, J. Lemmer (Eds.), Uncertainty in Artificial Intelligence 6, North-Holland (1991), pp. 347-374
    Google Scholar
    [16]
    J.Y. Halpern, R. Fagin
    Two views of belief: belief as generalized probability and belief as evidence
    Artif. Intell., 54 (3) (1992), pp. 275-317
    View PDF View article View in Scopus Google Scholar
    [17]
    D. Harmanec, G.J. Klir
    Measuring total uncertainty in Dempster–Shafer theory: a novel approach
    Int. J. Gen. Syst., 22 (4) (1994), pp. 405-419
    CrossRef View in Scopus Google Scholar
    [18]
    D. Harmanec, G. Resconi, G.J. Klir, Y. Pin
    On the computation of uncertainty measure in Dempster–Shafer theory
    Int. J. Gen. Syst., 25 (2) (1996), pp. 153-163
    CrossRef View in Scopus Google Scholar
    [19]
    R.V.L. Hartley
    Transmission of information
    Bell Syst. Tech. J., 7 (3) (1928), pp. 535-563
    CrossRef View in Scopus Google Scholar
    [20]
    U. Höhle
    Entropy with respect to plausibility measures
    Proceedings of the 12th IEEE Symposium on Multiple-Valued Logic (1982), pp. 167-169
    Google Scholar
    [21]
    R. Jiroušek, P.P. Shenoy
    Entropy of belief functions in the Dempster–Shafer theory: a new perspective
    J. Vejnarová, V. Kratochvíl (Eds.), Belief Functions: Theory and Applications, Lect. Notes Comput. Sci., vol. 9861, Springer International Publishing, Switzerland (2016), pp. 3-13
    CrossRef View in Scopus Google Scholar
    [22]
    A.-L. Jousselme, C. Liu, D. Grenier, E. Bossé
    Measuring ambiguity in the evidence theory
    IEEE Trans. Syst. Man Cybern., Part A, Syst. Hum., 36 (5) (2006), pp. 890-903
    View in Scopus Google Scholar
    [23]
    G.J. Klir, H.W. Lewis III
    Remarks on “Measuring ambiguity in the evidence theory”
    IEEE Trans. Syst. Man Cybern., Part A, Syst. Hum., 38 (4) (2008), pp. 995-999
    CrossRef View in Scopus Google Scholar
    [24]
    G.J. Klir, B. Parviz
    A note on the measure of discord
    D. Dubois, M.P. Wellman, B. D'Ambrosio, P. Smets (Eds.), Uncertainty in Artificial Intelligence: Proceedings of the Eighth Conference, Morgan Kaufmann (1992), pp. 138-141
    View PDF View article Google Scholar
    [25]
    G.J. Klir, A. Ramer
    Uncertainty in the Dempster–Shafer theory: a critical re-examination
    Int. J. Gen. Syst., 18 (2) (1990), pp. 155-166
    CrossRef View in Scopus Google Scholar
    [26]
    G.J. Klir, M.J. Wierman
    Uncertainty-Based Information: Elements of Generalized Information Theory
    (2nd edition), Springer-Verlag (1999)
    Google Scholar
    [27]
    J. Kohlas, P.-A. Monney
    A Mathematical Theory of Hints: An Approach to the Dempster–Shafer Theory of Evidence
    Springer-Verlag, Berlin (1995)
    Google Scholar
    [28]
    M.T. Lamata, S. Moral
    Measures of entropy in the theory of evidence
    Int. J. Gen. Syst., 14 (4) (1988), pp. 297-305
    CrossRef View in Scopus Google Scholar
    [29]
    C. Liu, D. Grenier, A.-L. Jousselme, E. Bosse
    Reducing algorithm complexity for computing an aggregate uncertainty measure
    IEEE Trans. Syst. Man Cybern., Part A, Syst. Hum., 37 (5) (2007), pp. 669-679
    View in Scopus Google Scholar
    [30]
    D.J.C. MacKay
    Information Theory, Inference, and Learning Algorithms
    Cambridge University Press (2003)
    Google Scholar
    [31]
    Y. Maeda, H. Ichihashi
    An uncertainty measure under the random set inclusion
    Int. J. Gen. Syst., 21 (4) (1993), pp. 379-392
    CrossRef View in Scopus Google Scholar
    [32]
    Y. Maeda, H.T. Nguyen, H. Ichihashi
    Maximum entropy algorithms for uncertainty measures
    Int. J. Uncertain. Fuzziness Knowl.-Based Syst., 1 (1) (1993), pp. 69-93
    Google Scholar
    [33]
    D.A. Maluf
    Monotonicity of entropy computations in belief functions
    Intell. Data Anal., 1 (1997), pp. 207-213
    View PDF View article View in Scopus Google Scholar
    [34]
    A. Meyerowitz, F. Richman, E.A. Walker
    Calculating maximum-entropy probability densities for belief functions
    Int. J. Uncertain. Fuzziness Knowl.-Based Syst., 2 (4) (1994), pp. 377-389
    Google Scholar
    [35]
    H.T. Nguyen
    On entropy of random sets and possibility distributions
    J.C. Bezdek (Ed.), The Analysis of Fuzzy Information, CRC Press (1985), pp. 145-156
    Google Scholar
    [36]
    N.R. Pal, J.C. Bezdek, R. Hemasinha
    Uncertainty measures for evidential reasoning I: a review
    Int. J. Approx. Reason., 7 (3) (1992), pp. 165-183
    View PDF View article View in Scopus Google Scholar
    [37]
    N.R. Pal, J.C. Bezdek, R. Hemasinha
    Uncertainty measures for evidential reasoning II: a new measure of total uncertainty
    Int. J. Approx. Reason., 8 (1) (1993), pp. 1-16
    View PDF View article View in Scopus Google Scholar
    [38]
    M. Pouly, J. Kohlas, P.Y.A. Ryan
    Generalized information theory for hints
    Int. J. Approx. Reason., 54 (1) (2013), pp. 228-251
    View PDF View article View in Scopus Google Scholar
    [39]
    A. Ramer
    Uniqueness of information measure in the theory of evidence
    Fuzzy Sets Syst., 24 (2) (1987), pp. 183-196
    View PDF View article View in Scopus Google Scholar
    [40]
    A. Rényi
    Probability Theory
    North-Holland (1970)
    Google Scholar
    [41]
    L.J. Savage
    The Foundations of Statistics
    Wiley, New York, NY (1954)
    Google Scholar
    [42]
    G. Shafer
    A Mathematical Theory of Evidence
    Princeton University Press (1976)
    Google Scholar
    [43]
    G. Shafer
    Constructive probability
    Synthese, 48 (1) (1981), pp. 1-60
    View in Scopus Google Scholar
    [44]
    G. Shafer
    Perspectives on the theory and practice of belief functions
    Int. J. Approx. Reason., 4 (5–6) (1990), pp. 323-362
    View PDF View article View in Scopus Google Scholar
    [45]
    G. Shafer
    Rejoinders to comments on “Perspectives on the theory and practice of belief functions”
    Int. J. Approx. Reason., 6 (3) (1992), pp. 445-480
    View PDF View article View in Scopus Google Scholar
    [46]
    A. Shahpari, S.A. Seyedin
    A study on properties of Dempster–Shafer theory to probability theory transformations
    Iran. J. Electr. Electron. Eng., 11 (2) (2015), pp. 87-100
    View in Scopus Google Scholar
    [47]
    C.E. Shannon
    A mathematical theory of communication
    Bell Syst. Tech. J., 27 (1948), pp. 379-423
    623–656
    CrossRef View in Scopus Google Scholar
    [48]
    P. Smets
    Information content of an evidence
    Int. J. Man-Mach. Stud., 19 (1983), pp. 33-43
    View PDF View article View in Scopus Google Scholar
    [49]
    P. Smets
    Constructing the pignistic probability function in a context of uncertainty
    M. Henrion, R. Shachter, L.N. Kanal, J.F. Lemmer (Eds.), Uncertainty in Artificial Intelligence 5, North-Holland, Amsterdam (1990), pp. 29-40
    View in Scopus Google Scholar
    [50]
    P. Smets
    Decision making in a context where uncertainty is represented by belief functions
    R.P. Srivastava, T.J. Mock (Eds.), Belief Functions in Business Decisions, Physica-Verlag, Heidelberg (2002), pp. 316-332
    Google Scholar
    [51]
    P. Smets, R. Kennes
    The transferable belief model
    Artif. Intell., 66 (2) (1994), pp. 191-234
    View PDF View article View in Scopus Google Scholar
    [52]
    I.J. Taneja
    Generalized information measures and their applications
    http://www.mtm.ufsc.br/~taneja/book/book.html (2001)
    On-line book, 2nd edition
    Google Scholar
    [53]
    J. Vejnarová
    A few remarks on measures of uncertainty in Dempster–Shafer theory
    Preliminary Proceedings of the 2nd Workshop on Uncertainty Processing in Expert Systems (1991)
    Google Scholar
    [54]
    J. Vejnarová, G.J. Klir
    Measure of strife in Dempster–Shafer theory
    Int. J. Gen. Syst., 22 (1) (1993), pp. 25-42
    CrossRef View in Scopus Google Scholar
    [55]
    F. Voorbraak
    A computationally efficient approximation of Dempster–Shafer theory
    Int. J. Man-Mach. Stud., 30 (5) (1989), pp. 525-536
    View PDF View article View in Scopus Google Scholar
    [56]
    P. Walley
    Statistical Reasoning with Imprecise Probabilities
    Chapman & Hall (1991)
    Google Scholar
    [57]
    R. Yager
    Entropy and specificity in a mathematical theory of evidence
    Int. J. Gen. Syst., 9 (4) (1983), pp. 249-260
    CrossRef View in Scopus Google Scholar

Cited by (118)

    DBE: Dynamic belief entropy for evidence theory with its application in data fusion
    2023, Engineering Applications of Artificial Intelligence
    Show abstract
    BF-QC: Belief functions on quantum circuits
    2023, Expert Systems with Applications
    Show abstract
    Logical entropy and aggregation of fuzzy orthopartitions
    2023, Fuzzy Sets and Systems
    Show abstract
    A numerical comparative study of uncertainty measures in the Dempster–Shafer evidence theory
    2023, Information Sciences
    Show abstract
    Entropy for evaluation of Dempster-Shafer belief function models
    2022, International Journal of Approximate Reasoning
    Show abstract
    BIM-AFA: Belief information measure-based attribute fusion approach in improving the quality of uncertain data
    2022, Information Sciences
    Show abstract

View all citing articles on Scopus

☆

    This paper is part of the Virtual special issue on Belief Functions: Theory and Applications, edited by Jiřina Vejnarová and Václav Kratochvíl.

© 2017 Elsevier Inc.
Part of special issue
Belief Functions: Theory and Applications
Edited by Jirina Vejnarova, Vaclav Kratochvil
View special issue
Recommended articles

    A correlation coefficient for belief functions
    International Journal of Approximate Reasoning, Volume 103, 2018, pp. 94-106
    Wen Jiang
    View PDF
    Idempotent conjunctive and disjunctive combination of belief functions by distance minimization
    International Journal of Approximate Reasoning, Volume 92, 2018, pp. 32-48
    John Klein , …, Olivier Colot
    View PDF
    Dempster–Shafer belief structures for decision making under uncertainty
    Knowledge-Based Systems, Volume 80, 2015, pp. 58-66
    Ronald R. Yager , Naif Alajlan
    View PDF

Show 3 more articles
Article Metrics
Citations

    Citation Indexes: 110 

Captures

    Readers: 34 

plumX logo
View details
Elsevier logo with wordmark

    About ScienceDirect
    Remote access
    Shopping cart
    Advertise
    Contact and support
    Terms and conditions
    Privacy policy 

We use cookies to help provide and enhance our service and tailor content and ads. By continuing you agree to the use of cookies .

Copyright © 2023 Elsevier B.V. or its licensors or contributors. ScienceDirect® is a registered trademark of Elsevier B.V.

ScienceDirect® is a registered trademark of Elsevier B.V.
RELX group home page
logo icon
